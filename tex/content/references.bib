
@inproceedings{abbeel2004,
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Abbeel, Pieter and Ng, Andrew Y.},
  year = {2004},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015430},
  abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using ``inverse reinforcement learning'' to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
  file = {/Users/niels/Zotero/storage/WJ8G5QEN/Abbeel und Ng - 2004 - Apprenticeship learning via inverse reinforcement .pdf},
  language = {en}
}

@article{abel,
  title = {Reinforcement {{Learning}} as a {{Framework}} for {{Ethical Decision Making}}},
  author = {Abel, David and MacGlashan, James and Littman, Michael L},
  pages = {8},
  abstract = {Emerging AI systems will be making more and more decisions that impact the lives of humans in a significant way. It is essential, then, that these AI systems make decisions that take into account the desires, goals, and preferences of other people, while simultaneously learning about what those preferences are. In this work, we argue that the reinforcementlearning framework achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper foundations for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. We define an idealized formalism for an ethical learner, and conduct experiments on two toy ethical dilemmas, demonstrating the soundness and flexibility of our approach. Lastly, we identify several critical challenges for future advancement in the area that can leverage our proposed framework.},
  file = {/Users/niels/Zotero/storage/FTRWER3T/Abel et al. - Reinforcement Learning as a Framework for Ethical .pdf},
  language = {en}
}

@misc{ai4people,
  title = {{{AI4People}} | {{Atomium}}-{{EISMD}}},
  author = {AI4People},
  abstract = {The Idea AI4People was established in June 2017 following a meeting between Prof Luciano Floridi, Director of the Digital Ethics Lab at the University of Oxford and Michelangelo Baracchi Bonvicini, President of Atomium-EISMD, that took place during the Atomium's Next Generation Internet Summit. ...},
  language = {en-US}
}

@inproceedings{akram2014,
  title = {Digital {{Trust}} - {{Trusted Computing}} and {{Beyond}}: {{A Position Paper}}},
  shorttitle = {Digital {{Trust}} - {{Trusted Computing}} and {{Beyond}}},
  booktitle = {2014 {{IEEE}} 13th {{International Conference}} on {{Trust}}, {{Security}} and {{Privacy}} in {{Computing}} and {{Communications}}},
  author = {Akram, Raja Naeem and Ko, Ryan K.L.},
  year = {2014},
  month = sep,
  pages = {884--892},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/TrustCom.2014.116},
  abstract = {Along with the invention of computers and interconnected networks, physical societal notions like security, trust, and privacy entered the digital environment. The concept of digital environments begins with the trust (established in the real world) in the organisation/individual that manages the digital resources. This concept evolved to deal with the rapid growth of the Internet, where it became impractical for entities to have prior offline (real world) trust. The evolution of digital trust took diverse approaches and now trust is defined and understood differently across heterogeneous domains. This paper looks at digital trust from the point of view of security and examines how valid trust approaches from other domains are now making their way into secure computing. The paper also revisits and analyses the Trusted Platform Module (TPM) along with associated technologies and their relevance in the changing landscape. We especially focus on the domains of cloud computing, mobile computing and cyber-physical systems. In addition, the paper also explores our proposals that are competing with and extending the traditional functionality of TPM specifications.},
  file = {/Users/niels/Zotero/storage/FSEBDXDJ/Akram und Ko - 2014 - Digital Trust - Trusted Computing and Beyond A Po.pdf},
  isbn = {978-1-4799-6513-7},
  language = {en}
}

@techreport{alsabah2019,
  title = {Blick in Die {{Blackbox}} - {{Nachvollziehbarkeit}} von {{KI}}-{{Algorithmen}} in Der {{Praxis}}},
  author = {Alsabah, Nabil},
  year = {2019},
  pages = {106},
  institution = {{Bitkom}},
  file = {/Users/niels/Zotero/storage/CQGRM36B/20191016_blick-in-die-blackbox.pdf}
}

@article{alshiekha,
  title = {Safe {{Reinforcement Learning}} via {{Shielding}}},
  author = {Alshiekh, Mohammed and Bloem, Roderick and Ehlers, Ruediger and Koenighofer, Bettina and Niekum, Scott and Topcu, Ufuk},
  pages = {10},
  abstract = {Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios.},
  file = {/Users/niels/Zotero/storage/Q248XRU7/Alshiekh et al. - Safe Reinforcement Learning via Shielding.pdf},
  language = {en}
}

@article{amodei2016,
  title = {Concrete {{Problems}} in {{AI Safety}}},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  year = {2016},
  month = jul,
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (``avoiding side effects'' and ``avoiding reward hacking''), an objective function that is too expensive to evaluate frequently (``scalable supervision''), or undesirable behavior during the learning process (``safe exploration'' and ``distributional shift''). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  archivePrefix = {arXiv},
  eprint = {1606.06565},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/KFX7QANJ/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf},
  journal = {arXiv:1606.06565 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@misc{ansi,
  title = {{{ANSI}}-{{American National Standards Institute}}},
  author = {ANSI},
  howpublished = {https://www.ansi.org/}
}

@book{arkin2009,
  title = {Governing Lethal Behavior in Autonomous Robots},
  author = {Arkin, Ronald C.},
  year = {2009},
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  file = {/Users/niels/Zotero/storage/ZXIGTL75/Arkin - 2009 - Governing lethal behavior in autonomous robots.pdf},
  isbn = {978-1-4200-8594-5},
  keywords = {Autonomous robots,Control systems,Military applications,Military ethics,Military robots,Moral and ethical aspects,Robotics,Robots,War},
  language = {en},
  lccn = {UG479 .A74 2009},
  note = {OCLC: ocn213318205}
}

@inproceedings{arvind2019,
  title = {Autonomous {{RL}}: {{Autonomous Vehicle Obstacle Avoidance}} in a {{Dynamic Environment}} Using {{MLP}}-{{SARSA Reinforcement Learning}}},
  shorttitle = {Autonomous {{RL}}},
  booktitle = {2019 {{IEEE}} 5th {{International Conference}} on {{Mechatronics System}} and {{Robots}} ({{ICMSR}})},
  author = {Arvind, C. S. and Senthilnath, J.},
  year = {2019},
  month = may,
  pages = {120--124},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/ICMSR.2019.8835462},
  abstract = {This paper presents a Multi-Layer Perceptron-State Action Reward State Action (MLP-SARSA) based reinforcement learning methodology for dynamic obstacle detection and avoidance for autonomous vehicle navigation. MLP-SARSA is an on-policy reinforcement learning approach, which gains information and rewards from the environment and helps the autonomous vehicle to avoid dynamic moving obstacles. MLP with SARSA provides a significant advantage over dynamic environment compared to other traditional reinforcement algorithms. In this study, a MLP-SARSA model is trained in a complex urban simulation environment with dynamic obstacles using the pygame library. Experimental results show that the trained MLP-SARSA can navigate the autonomous vehicle in a dynamic environment with more confidences than traditional Q-learning and SARSA reinforcement algorithms.},
  file = {/Users/niels/Zotero/storage/MY5CJMYY/Arvind and Senthilnath - 2019 - Autonomous RL Autonomous Vehicle Obstacle Avoidan.pdf},
  isbn = {978-1-72812-223-6},
  language = {en}
}

@article{avizienis2004,
  title = {Basic Concepts and Taxonomy of Dependable and Secure Computing},
  author = {Avizienis, A. and Laprie, J.-C. and Randell, B. and Landwehr, C.},
  year = {2004},
  month = jan,
  volume = {1},
  pages = {11--33},
  issn = {1545-5971},
  doi = {10.1109/TDSC.2004.2},
  abstract = {This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.},
  file = {/Users/niels/Zotero/storage/BCY7JNEA/Avizienis et al. - 2004 - Basic concepts and taxonomy of dependable and secu.pdf},
  journal = {IEEE Transactions on Dependable and Secure Computing},
  language = {en},
  number = {1}
}

@book{balzert2011,
  title = {{Lehrbuch der Softwaretechnik Entwurf, Implementierung, Installation und Betrieb}},
  author = {Balzert, Helmut and Liggesmeyer, Peter},
  year = {2011},
  publisher = {{Spektrum Akademischer Verl}},
  address = {{Heidelberg}},
  file = {/Users/niels/Zotero/storage/ZT7WY8D7/Balzert und Liggesmeyer - 2011 - Lehrbuch der Softwaretechnik Entwurf, Implementier.pdf},
  isbn = {978-3-8274-2246-0},
  language = {German},
  note = {OCLC: 1078359165}
}

@inproceedings{baracaldo2017,
  title = {Mitigating {{Poisoning Attacks}} on {{Machine Learning Models}}: {{A Data Provenance Based Approach}}},
  shorttitle = {Mitigating {{Poisoning Attacks}} on {{Machine Learning Models}}},
  booktitle = {Proceedings of the 10th {{ACM Workshop}} on {{Artificial Intelligence}} and {{Security}} - {{AISec}} '17},
  author = {Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Safavi, Jaehoon Amir},
  year = {2017},
  pages = {103--110},
  publisher = {{ACM Press}},
  address = {{Dallas, Texas, USA}},
  doi = {10.1145/3128572.3140450},
  abstract = {The use of machine learning models has become ubiquitous. Their predictions are used to make decisions about healthcare, security, investments and many other critical applications. Given this pervasiveness, it is not surprising that adversaries have an incentive to manipulate machine learning models to their advantage. One way of manipulating a model is through a poisoning or causative attack in which the adversary feeds carefully crafted poisonous data points into the training set. Taking advantage of recently developed tamper-free provenance frameworks, we present a methodology that uses contextual information about the origin and transformation of data points in the training set to identify poisonous data, thereby enabling online and regularly re-trained machine learning applications to consume data sources in potentially adversarial environments. To the best of our knowledge, this is the first approach to incorporate provenance information as part of a filtering algorithm to detect causative attacks. We present two variations of the methodology\textendash{}one tailored to partially trusted data sets and the other to fully untrusted data sets. Finally, we evaluate our methodology against existing methods to detect poison data and show an improvement in the detection rate.},
  file = {/Users/niels/Zotero/storage/XTY9M9B6/Baracaldo et al. - 2017 - Mitigating Poisoning Attacks on Machine Learning M.pdf},
  isbn = {978-1-4503-5202-4},
  language = {en}
}

@article{baum2017,
  title = {On the Promotion of Safe and Socially Beneficial Artificial Intelligence},
  author = {Baum, Seth D.},
  year = {2017},
  month = nov,
  volume = {32},
  pages = {543--551},
  issn = {0951-5666, 1435-5655},
  doi = {10.1007/s00146-016-0677-0},
  abstract = {This paper discusses means for promoting artificial intelligence (AI) that is designed to be safe and beneficial for society (or simply ``beneficial AI''). The promotion of beneficial AI is a social challenge because it seeks to motivate AI developers to choose beneficial AI designs. Currently, the AI field is focused mainly on building AIs that are more capable, with little regard to social impacts. Two types of measures are available for encouraging the AI field to shift more toward building beneficial AI. Extrinsic measures impose constraints or incentives on AI researchers to induce them to pursue beneficial AI even if they do not want to. Intrinsic measures encourage AI researchers to want to pursue beneficial AI. Prior research focuses on extrinsic measures, but intrinsic measures are at least as important. Indeed, intrinsic factors can determine the success of extrinsic measures. Efforts to promote beneficial AI must consider intrinsic factors by studying the social psychology of AI research communities.},
  file = {/Users/niels/Zotero/storage/JFYHKSMG/Baum - 2017 - On the promotion of safe and socially beneficial a.pdf},
  journal = {AI \& SOCIETY},
  language = {en},
  number = {4}
}

@book{bendel2019,
  title = {{Handbuch Maschinenethik}},
  editor = {Bendel, Oliver},
  year = {2019},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-17483-5},
  file = {/Users/niels/Zotero/storage/AY95TIYJ/Bendel - 2019 - Handbuch Maschinenethik.pdf},
  isbn = {978-3-658-17482-8 978-3-658-17483-5},
  language = {de}
}

@misc{bitkom,
  title = {{Bitkom e.V.}},
  author = {{bitkom}},
  abstract = {Der Digitalverband Deutschlands | Bitkom - Bundesverband Informationswirtschaft, Telekommunikation und neue Medien e. V.},
  howpublished = {https://www.bitkom.org/},
  language = {de}
}

@inproceedings{bou-ammar2010,
  title = {Controller Design for Quadrotor {{UAVs}} Using Reinforcement Learning},
  booktitle = {2010 {{IEEE International Conference}} on {{Control Applications}}},
  author = {{Bou-Ammar}, Haitham and Voos, Holger and Ertel, Wolfgang},
  year = {2010},
  month = sep,
  pages = {2130--2135},
  publisher = {{IEEE}},
  address = {{Yokohama, Japan}},
  doi = {10.1109/CCA.2010.5611206},
  abstract = {Quadrotor UAVs are one of the most preferred type of small unmanned aerial vehicles because of the very simple mechanical construction and propulsion principle. However, the nonlinear dynamic behavior requires a rather advanced stabilizing control of these vehicles. One possible approach that relaxes the difficult task of nonlinear control design is the application of a learning algorithm that allows the training of suitable control actions. Here we apply reinforcement learning as one form of unsupervised learning. In this paper, we first propose a nonlinear autopilot for quadrotor UAVs based on feedback linearization. This controller is then compared to an autopilot which has been learned by reinforcement learning using fitted value iteration with regard to design effort and performance. First simulation and experimental results underline the outcome of this comparison.},
  file = {/Users/niels/Zotero/storage/BLLJRIMK/Bou-Ammar et al. - 2010 - Controller design for quadrotor UAVs using reinfor.pdf},
  isbn = {978-1-4244-5362-7},
  language = {en}
}

@book{brink2013,
  title = {{Anfertigung wissenschaftlicher Arbeiten: ein prozessorientierter Leitfaden zur Erstellung von Bachelor-, Master- und Diplomarbeiten}},
  shorttitle = {{Anfertigung wissenschaftlicher Arbeiten}},
  author = {Brink, Alfred},
  year = {2013},
  edition = {4., korrigierte und aktualisierte Aufl},
  publisher = {{Springer Gabler}},
  address = {{Wiesbaden}},
  file = {/Users/niels/Zotero/storage/UR4KBJDG/Brink - 2013 - Anfertigung wissenschaftlicher Arbeiten ein proze.pdf},
  isbn = {978-3-8349-4396-5 978-3-8349-4397-2},
  language = {de},
  note = {OCLC: 812137497},
  series = {{Lehrbuch}}
}

@article{brockman2016,
  title = {Openai Gym},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.01540}
}

@inbook{broy2013,
  title = {{Projekt- und Produktlebenszyklus von Software}},
  booktitle = {{Projektorganisation und Management im Software Engineering}},
  author = {Broy, Manfred and Kuhrmann, Marco},
  year = {2013},
  pages = {61--83},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-29290-3_3},
  collaborator = {Broy, Manfred and Kuhrmann, Marco},
  file = {/Users/niels/Zotero/storage/7FGDPF28/Broy und Kuhrmann - 2013 - Projekt- und Produktlebenszyklus von Software.pdf},
  isbn = {978-3-642-29289-7 978-3-642-29290-3},
  language = {de}
}

@misc{businesswire2019,
  title = {Global {{Autonomous}}/{{Driverless Car Market Forecasts}} to 2024: {{Semi}}-{{Autonomous Vehicles Dominating}} the {{Market}} - {{ResearchAndMarkets}}.Com},
  shorttitle = {Global {{Autonomous}}/{{Driverless Car Market Forecasts}} to 2024},
  author = {{businesswire}},
  year = {2019},
  month = may,
  abstract = {The},
  howpublished = {https://www.businesswire.com/news/home/20190524005297/en/Global-AutonomousDriverless-Car-Market-Forecasts-2024-Semi-Autonomous},
  language = {en}
}

@article{byrne2018,
  title = {Making {{Drones}} to {{Kill Civilians}}: {{Is}} It {{Ethical}}?},
  shorttitle = {Making {{Drones}} to {{Kill Civilians}}},
  author = {Byrne, Edmund F.},
  year = {2018},
  month = jan,
  volume = {147},
  pages = {81--93},
  issn = {0167-4544, 1573-0697},
  doi = {10.1007/s10551-015-2950-4},
  abstract = {A drone industry has emerged in the US, initially funded almost exclusively for military applications. There are now also other uses both governmental and commercial (in the US and abroad). Many military drones are still being made, however, especially for surveillance and targeted killings. Regarding the latter, this essay calls into question their legality and morality. It recognizes that the issues are complex and controversial, but less so as to the killing of non-combatant civilians. The government using drones for targeted killings maintains secrecy and appeals to non-traditional justifications. Most scholars who assess these killer drone practices support citizen immunity, either by favoring a modified just war theory that prioritizes civilians' right to life or by challenging official deviations from applicable laws. They accordingly declare such killing immoral if not a war crime. The manufacturers of these killer drones are not themselves the killers, but they are abetters, i.e., sine qua non facilitators. So, I argue that any company concerned about its corporate social responsibility should cease manufacturing them.},
  file = {/Users/niels/Zotero/storage/J8ZIMZMX/Byrne - 2018 - Making Drones to Kill Civilians Is it Ethical.pdf},
  journal = {Journal of Business Ethics},
  language = {en},
  number = {1}
}

@book{castano,
  title = {Practical {{Artificial Intelligence}}},
  author = {Casta{\~n}o, Arnaldo P{\'e}rez},
  file = {/Users/niels/Zotero/storage/ZZV32SUA/Castaño - Practical Artificial Intelligence.pdf},
  language = {en}
}

@article{cervantes2016,
  title = {Autonomous {{Agents}} and {{Ethical Decision}}-{{Making}}},
  author = {Cervantes, Jos{\'e}-Antonio and Rodr{\'i}guez, Luis-Felipe and L{\'o}pez, Sonia and Ramos, F{\'e}lix and Robles, Francisco},
  year = {2016},
  month = apr,
  volume = {8},
  pages = {278--296},
  issn = {1866-9956, 1866-9964},
  doi = {10.1007/s12559-015-9362-8},
  file = {/Users/niels/Zotero/storage/9SCMFKCX/Cervantes et al. - 2016 - Autonomous Agents and Ethical Decision-Making.pdf},
  journal = {Cognitive Computation},
  language = {en},
  number = {2}
}

@incollection{chatila2019,
  title = {The {{IEEE Global Initiative}} on {{Ethics}} of {{Autonomous}} and {{Intelligent Systems}}},
  booktitle = {Robotics and {{Well}}-{{Being}}},
  author = {Chatila, Raja and Havens, John C.},
  editor = {Aldinhas Ferreira, Maria Isabel and Silva Sequeira, Jo{\~a}o and Singh Virk, Gurvinder and Tokhi, Mohammad Osman and E. Kadar, Endre},
  year = {2019},
  volume = {95},
  pages = {11--16},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-12524-0_2},
  file = {/Users/niels/Zotero/storage/4RCP2DG2/Chatila and Havens - 2019 - The IEEE Global Initiative on Ethics of Autonomous.pdf},
  isbn = {978-3-030-12523-3 978-3-030-12524-0},
  language = {en}
}

@misc{claire,
  title = {{{CLAIRE}}},
  author = {CLAIRE},
  abstract = {Confederation of Laboratories for Artificial Intelligence Research in Europe},
  howpublished = {https://claire-ai.org/},
  journal = {CLAIRE},
  language = {en-US}
}

@article{coeckelbergh2013,
  title = {Drones, Information Technology, and Distance: Mapping the Moral Epistemology of Remote Fighting},
  shorttitle = {Drones, Information Technology, and Distance},
  author = {Coeckelbergh, Mark},
  year = {2013},
  month = jun,
  volume = {15},
  pages = {87--98},
  issn = {1388-1957, 1572-8439},
  doi = {10.1007/s10676-013-9313-6},
  abstract = {Ethical reflection on drone fighting suggests that this practice does not only create physical distance, but also moral distance: far removed from one's opponent, it becomes easier to kill. This paper discusses this thesis, frames it as a moral-epistemological problem, and explores the role of information technology in bridging and creating distance. Inspired by a broad range of conceptual and empirical resources including ethics of robotics, psychology, phenomenology, and media reports, it is first argued that drone fighting, like other long-range fighting, creates epistemic and moral distance in so far as `screenfighting' implies the disappearance of the vulnerable face and body of the opponent and thus removes moral-psychological barriers to killing. However, the paper also shows that this influence is at least weakened by current surveillance technologies, which make possible a kind of `empathic bridging' by which the fighter's opponent on the ground is re-humanized, re-faced, and re-embodied. This `mutation' or unintended `hacking' of the practice is a problem for drone pilots and for those who order them to kill, but revealing its moral-epistemic possibilities opens up new avenues for imagining morally better ways of technologymediated fighting.},
  file = {/Users/niels/Zotero/storage/6ZEAZML2/Coeckelbergh - 2013 - Drones, information technology, and distance mapp.pdf},
  journal = {Ethics and Information Technology},
  language = {en},
  number = {2}
}

@article{cointe,
  title = {Ethical {{Judgment}} of {{Agents}}' {{Behaviors}} in {{Multi}}-{{Agent Systems}}},
  author = {Cointe, Nicolas and Bonnet, Gr{\'e}gory and Boissier, Olivier},
  pages = {9},
  abstract = {The increasing use of multi-agent technologies in various areas raises the necessity of designing agents that judge ethical behaviors in context. This is why several works integrate ethical concepts in agents' decision making processes. However, those approaches consider mainly an agent-centered perspective, letting aside the fact that agents are in interaction with other artificial agents or human beings that can use other ethical concepts. In this article, we address the problem of producing ethical behaviors from a multi-agent perspective. To this end, we propose a model of ethical judgment an agent can use in order to judge the ethical dimension of both its own behavior and the other agents' behaviors. This model is based on a rationalist and explicit approach that distinguishes theory of good and theory of right. A proof-of-concept implemented in Answer Set Programming and based on a simple scenario is given to illustrate those functionalities.},
  file = {/Users/niels/Zotero/storage/2D2Q9UML/Cointe et al. - Ethical Judgment of Agents’ Behaviors in Multi-Age.pdf},
  language = {en}
}

@article{cremers,
  title = {{HANDLUNGSFELDER AUS PHILOSOPHISCHER, ETHISCHER, RECHTLICHER UND TECHNOLOGISCHER SICHT ALS GRUNDLAGE F{\"U}R EINE ZERTIFIZIERUNG VON K{\"U}NSTLICHER INTELLIGENZ}},
  author = {Cremers, Dr Armin B and Englander, Dr Alex and Gabriel, Dr Markus and Hecker, Dr Dirk and Poretschkin, Dr Maximilian and Rosenzweig, Julia and Sicking, Joachim and Volmer, Dr Julia and Voosholz, Jan and Voss, Dr Angelika and Wrobel, Dr Stefan},
  pages = {21},
  file = {/Users/niels/Zotero/storage/INRD6AFD/Cremers et al. - HANDLUNGSFELDER AUS PHILOSOPHISCHER, ETHISCHER, RE.pdf},
  language = {de}
}

@article{degreef2015,
  title = {Design for Responsibility: Safeguarding Moral Perception via a Partnership Architecture},
  shorttitle = {Design for Responsibility},
  author = {{de Greef}, Tjerk and Leveringhaus, Alex},
  year = {2015},
  month = aug,
  volume = {17},
  pages = {319--328},
  issn = {1435-5558, 1435-5566},
  doi = {10.1007/s10111-015-0329-z},
  abstract = {Advanced warfare technologies (AWT) create unprecedented capabilities to control the delivery of military force up to the point, some argue, that we are loosing humanity. But dependence on them generates difficult moral challenges impacting the decision-making process, which are only beginning to be addressed. In order to arrive at an informed opinion about the impact of AWT on decision-making, we need to know more about what AWTs are and how they operate. We provide a short overview of the different types of AWTs and discuss the key principles that underlie Humanitarian Law. We also discuss the impact of physical distance and increased levels of autonomy on AWT and discuss the challenges posed to moral perception. Before such systems can be deployed, we need to rest assured that their usage enhances, rather than undermines, human decision-making capacities. There are important choices to be made, and sound design is `design for responsibility'. As a solution, we therefore propose the partnership architecture that embeds concurrent views of the world and working agreements, ensuring that operators use appropriate information in the decision-making process.},
  file = {/Users/niels/Zotero/storage/GGT3MTPU/de Greef and Leveringhaus - 2015 - Design for responsibility safeguarding moral perc.pdf},
  journal = {Cognition, Technology \& Work},
  language = {en},
  number = {3}
}

@article{deswarte2019,
  title = {Artificial Intelligence, Ethics and Human Values: The Cases of Military Drones and Companion Robots},
  shorttitle = {Artificial Intelligence, Ethics and Human Values},
  author = {{de Swarte}, Thibault and Boufous, Omar and Escalle, Paul},
  year = {2019},
  month = sep,
  volume = {24},
  pages = {291--296},
  issn = {1433-5298, 1614-7456},
  doi = {10.1007/s10015-019-00525-1},
  abstract = {Can artificial intelligence (AI) be more ethical than human intelligence? Can it respect human values better than a human? This article examines some issues raised by the AI with respect to ethics. The utilitarian approach can be a solution, especially the one that uses agent-based theory. We have chosen two extreme cases: combat drones, vectors of death, and life supporting companion robots. The ethics of AI and unmanned aerial vehicles (UAV) must be studied on the basis of military ethics and human values when fighting. Despite the fact that they are not programmed to hurt humans or harm their dignity, companion robots can potentially endanger their social, moral as well as their physical integrity. An important ethical condition is that companion robots help the nursing staff to take better care of patients while not replacing them.},
  file = {/Users/niels/Zotero/storage/FF4A7TVM/de Swarte et al. - 2019 - Artificial intelligence, ethics and human values .pdf},
  journal = {Artificial Life and Robotics},
  language = {en},
  number = {3}
}

@book{dhariwal2017,
  title = {Openai Baselines},
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  year = {2017}
}

@misc{din,
  title = {{{DIN}} - {{Deutsches Institut}} F{\"u}r {{Normung}}},
  author = {DIN},
  howpublished = {https://www.din.de/de}
}

@article{ding,
  title = {Deciphering {{China}}'s {{AI Dream}}},
  author = {Ding, Jeffrey},
  pages = {44},
  file = {/Users/niels/Zotero/storage/LCNY8294/Ding - Deciphering China’s AI Dream.pdf},
  language = {en}
}

@book{duttge2009,
  title = {{Das Ich und sein Gehirn: die Herausforderung der neurobiologischen Forschung f{\"u}r das (Straf-)Recht}},
  shorttitle = {{Das Ich und sein Gehirn}},
  editor = {Duttge, Gunnar and {Institut f{\"u}r Kriminalwissenschaften} and Kriminalwissenschaftliches Kolloquium},
  year = {2009},
  publisher = {{Univ.-Verl. G{\"o}ttingen}},
  address = {{G{\"o}ttingen}},
  abstract = {In den letzten Jahren hat die Debatte um den freien Willen an Intensit{\"a}t gewonnen und nicht nur in den Feuilletons ihren Niederschlag gefunden. Die Phase der hitzigen Auseinandersetzung ist inzwischen einem st{\"a}rker sachbezogenen, vom Bem{\"u}hen um Verstehen und Vermittlung gepr{\"a}gten interdisziplin{\"a}ren Diskurs gewichen. Doch noch immer bestehen Missverst{\"a}ndnisse, Fehlannahmen oder Unklarheiten,die zuweilen flankiert sind von einer Haltung der Diskursverweigerung. Der vorliegende Band tr{\"a}gt dazu bei, die Selbstverst{\"a}ndnisse und Sichtweisen der betroffenen Disziplinen klar herauszustellen, um auf diese Weise das Verbindende ebenso wie das Trennende besser erkennen zu k{\"o}nnen: Philosophie und Neurowissenschaften, Strafrechtswissenschaft und Strafrechtspraxis, und nicht zuletzt die Forensische Psychiatrie treten in das n{\"o}tige Zwiegespr{\"a}ch ein. Der Band nimmt nicht in Anspruch, endg{\"u}ltige Antworten zu pr{\"a}sentieren, sondern liefert Kl{\"a}rungen, die es dem interessierten Leser erlauben, sich eine reflektierte Meinung zu bilden. Die Beitr{\"a}ge sind aus dem ersten Workshop des Instituts f{\"u}r Kriminalwissenschaften der Georg-August-Universit{\"a}t G{\"o}ttingen im Jahr 2007 hervorgegangen. (Quelle: Text Verlagseinband / Verlag)},
  file = {/Users/niels/Zotero/storage/UV8SXH8H/Duttge et al. - 2009 - Das Ich und sein Gehirn die Herausforderung der n.pdf},
  isbn = {978-3-941875-01-2},
  language = {de},
  note = {OCLC: 436303860},
  number = {7},
  series = {{G{\"o}ttinger Studien zu den Kriminalwissenschaften}}
}

@misc{emelc-wg,
  title = {P7000 - {{Model Process}} for {{Addressing Ethical Concerns During System Design}}},
  author = {{EMELC-WG}, IEEE}
}

@article{feldman2019,
  title = {Integrating {{Artificial Intelligence}} into {{Weapon Systems}}},
  author = {Feldman, Philip and Dant, Aaron and Massey, Aaron},
  year = {2019},
  month = may,
  abstract = {The integration of Artificial Intelligence (AI) into weapon systems is one of the most consequential tactical and strategic decisions in the history of warfare. Current AI development is a remarkable combination of accelerating capability, hidden decision mechanisms, and decreasing costs. Implementation of these systems is in its infancy and exists on a spectrum from resilient and flexible to simplistic and brittle. Resilient systems should be able to effectively handle the complexities of a high-dimensional battlespace. Simplistic AI implementations could be manipulated by an adversarial AI that identifies and exploits their weaknesses. In this paper, we present a framework for understanding the development of dynamic AI/ML systems that interactively and continuously adapt to their user's needs. We explore the implications of increasingly capable AI in the kill chain and how this will lead inevitably to a fully automated, always on system, barring regulation by treaty. We examine the potential of total integration of cyber and physical security and how this likelihood must inform the development of AI-enabled systems with respect to the ``fog of war'', human morals, and ethics.},
  archivePrefix = {arXiv},
  eprint = {1905.03899},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/IFFGFEMA/Feldman et al. - 2019 - Integrating Artificial Intelligence into Weapon Sy.pdf},
  journal = {arXiv:1905.03899 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Robotics,I.2.0,J.7,K.4.1},
  language = {en},
  primaryClass = {cs}
}

@misc{fraunhofer,
  title = {{Roberta4Home - Fraunhofer IAIS}},
  author = {{fraunhofer}, IAIS},
  file = {/Users/niels/Zotero/storage/Q2EXZGHF/presseinformation-200408.html},
  howpublished = {https://www.iais.fraunhofer.de/de/presse/presseinformationen/presseinformationen-2020/presseinformation-200408.html},
  journal = {Fraunhofer-Institut f{\"u}r Intelligente Analyse- und Informationssysteme IAIS},
  language = {de}
}

@article{gilberta,
  title = {{Vertrauen als Gegenstand der {\"o}konomischen Theorie}},
  author = {Gilbert, Dirk Ulrich},
  pages = {48},
  file = {/Users/niels/Zotero/storage/P35EMUDT/Gilbert - Vertrauen als Gegenstand der ökonomischen Theorie.pdf},
  language = {de}
}

@article{gottesman2018,
  title = {Evaluating {{Reinforcement Learning Algorithms}} in {{Observational Health Settings}}},
  author = {Gottesman, Omer and Johansson, Fredrik and Meier, Joshua and Dent, Jack and Lee, Donghun and Srinivasan, Srivatsan and Zhang, Linying and Ding, Yi and Wihl, David and Peng, Xuefeng and Yao, Jiayu and Lage, Isaac and Mosch, Christopher and Lehman, Li-wei H. and Komorowski, Matthieu and Komorowski, Matthieu and Faisal, Aldo and Celi, Leo Anthony and Sontag, David and {Doshi-Velez}, Finale},
  year = {2018},
  month = may,
  abstract = {Much attention has been devoted recently to the development of machine learning algorithms with the goal of improving treatment policies in healthcare. Reinforcement learning (RL) is a sub-field within machine learning that is concerned with learning how to make sequences of decisions so as to optimize long-term effects. Already, RL algorithms have been proposed to identify decision-making strategies for mechanical ventilation, sepsis management and treatment of schizophrenia. However, before implementing treatment policies learned by black-box algorithms in high-stakes clinical decision problems, special care must be taken in the evaluation of these policies. In this document, our goal is to expose some of the subtleties associated with evaluating RL algorithms in healthcare. We aim to provide a conceptual starting point for clinical and computational researchers to ask the right questions when designing and evaluating algorithms for new ways of treating patients. In the following, we describe how choices about how to summarize a history, variance of statistical estimators, and confounders in more ad-hoc measures can result in unreliable, even misleading estimates of the quality of a treatment policy. We also provide suggestions for mitigating these effects---for while there is much promise for mining observational health data to uncover better treatment policies, evaluation must be performed thoughtfully.},
  archivePrefix = {arXiv},
  eprint = {1805.12298},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/THVLWQIA/Gottesman et al. - 2018 - Evaluating Reinforcement Learning Algorithms in Ob.pdf},
  journal = {arXiv:1805.12298 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@book{gramlich2007,
  title = {{Herausforderungen einer zukunftsorientierten Unternehmenspolitik}},
  editor = {Gramlich, Dieter and Tr{\"a}ger, Manfred},
  year = {2007},
  publisher = {{DUV}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-8350-9621-9},
  file = {/Users/niels/Zotero/storage/YY7XC8BZ/Gramlich and Träger - 2007 - Herausforderungen einer zukunftsorientierten Unter.pdf},
  isbn = {978-3-8350-0811-3},
  language = {de}
}

@article{hall2007,
  title = {Self-Improving {{AI}}: An {{Analysis}}},
  shorttitle = {Self-Improving {{AI}}},
  author = {Hall, John Storrs},
  year = {2007},
  month = oct,
  volume = {17},
  pages = {249--259},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-007-9065-3},
  abstract = {Self-improvement was one of the aspects of AI proposed for study in the 1956 Dartmouth conference. Turing proposed a ``child machine'' which could be taught in the human manner to attain adult human-level intelligence. In latter days, the contention that an AI system could be built to learn and improve itself indefinitely has acquired the label of the bootstrap fallacy. Attempts in AI to implement such a system have met with consistent failure for half a century. Technological optimists, however, have maintained that a such system is possible, producing, if implemented, a feedback loop that would lead to a rapid exponential increase in intelligence. We examine the arguments for both positions and draw some conclusions.},
  file = {/Users/niels/Zotero/storage/BPNQYWS8/Hall - 2007 - Self-improving AI an Analysis.pdf},
  journal = {Minds and Machines},
  language = {en},
  number = {3}
}

@book{hartmann2011,
  title = {{Die Praxis des Vertrauens}},
  author = {Hartmann, Martin},
  year = {2011},
  publisher = {{Suhrkamp}},
  address = {{Berlin}},
  file = {/Users/niels/Zotero/storage/IXFAV4N2/Hartmann - 2011 - Die Praxis des Vertrauens.pdf},
  isbn = {978-3-518-76201-1 978-3-518-76200-4},
  language = {de},
  note = {OCLC: 871619280}
}

@book{hartmann2013,
  title = {{Literaturkompass Politikwissenschaft: Einf{\"u}hrung in die politikwissenschaftliche Literatur}},
  shorttitle = {{Literaturkompass Politikwissenschaft}},
  author = {Hartmann, J{\"u}rgen and Sanders, Luise},
  year = {2013},
  publisher = {{Springer VS}},
  address = {{Wiesbaden}},
  abstract = {Diese Einf{\"u}hrung schl{\"a}gt f{\"u}r Studierende und Dozenten eine Schneise in die politikwissenschaftliche Literatur und bietet einen {\"U}berblick {\"u}ber die wichtigen Schl{\"u}ssel-, Lehr- und Forschungstexte der Politikwissenschaft und deren Teilgebiete. Inhalt : Einleitung.- Einf{\"u}hrungs- und {\"U}bersichtswerke.- Politische Systeme.- Internationale Beziehungen.- Europ{\"a}ische Union.- Politische Theorie und Ideengeschichte.- Methoden.- Zeitschriften},
  file = {/Users/niels/Zotero/storage/D7YNM4WZ/Hartmann and Sanders - 2013 - Literaturkompass Politikwissenschaft Einführung i.pdf},
  isbn = {978-3-658-00162-9 978-3-658-00163-6},
  language = {de},
  note = {OCLC: 856791100},
  series = {{Lehrbuch}}
}

@article{hawkins,
  title = {Serious Safety Lapses Led to {{Uber}}'s Fatal Self-Driving Crash, New Documents Suggest},
  author = {Hawkins, Andrew J.},
  language = {en}
}

@inproceedings{heckman1998,
  title = {Liability for Autonomous Agent Design},
  booktitle = {Proceedings of the Second International Conference on {{Autonomous}} Agents  - {{AGENTS}} '98},
  author = {Heckman, Carey and Wobbrock, Jacob O.},
  year = {1998},
  pages = {392--399},
  publisher = {{ACM Press}},
  address = {{Minneapolis, Minnesota, United States}},
  doi = {10.1145/280765.280869},
  abstract = {Though exciting scientifically, autonomous agent design can result in legal liability. This paper surveys those legal concerns, focusing on issues arising from the unique qualities of agents not found in conventional software. Informed designers can more effectively reduce their liability exposure and influence emerging agent liability law and policies.},
  file = {/Users/niels/Zotero/storage/Z3H55I5P/Heckman und Wobbrock - 1998 - Liability for autonomous agent design.pdf},
  isbn = {978-0-89791-983-8},
  language = {en}
}

@article{hellbardt1996,
  title = {{Die Ethik von Agenten}},
  author = {Hellbardt, G\&\#x000FC;nter},
  year = {1996},
  month = apr,
  volume = {19},
  pages = {87--90},
  issn = {0170-6012, 1432-122X},
  doi = {10.1007/s002870050021},
  file = {/Users/niels/Zotero/storage/2GTFCCUF/Hellbardt - 1996 - Die Ethik von Agenten.pdf},
  journal = {Informatik-Spektrum},
  language = {de},
  number = {2}
}

@article{henderson,
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  pages = {8},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  file = {/Users/niels/Zotero/storage/BPQTXMLY/Henderson et al. - Deep Reinforcement Learning that Matters.pdf},
  language = {en}
}

@article{herrmann,
  title = {{{RL}} 5: {{On}}-Policy and off-Policy Algorithms},
  author = {Herrmann, Michael},
  pages = {24},
  file = {/Users/niels/Zotero/storage/7JKKN9W3/Herrmann - RL 5 On-policy and oﬀ-policy algorithms.pdf},
  language = {en}
}

@article{hoffrage2019,
  title = {{Pers{\"o}nliche Verantwortung und Verantwortungs{\"u}bernahme in Systemen}},
  author = {Hoffrage, Ulrich},
  year = {2019},
  month = oct,
  volume = {54},
  pages = {4--9},
  issn = {0030-9338, 1613-7558},
  doi = {10.1007/s00608-019-0679-5},
  file = {/Users/niels/Zotero/storage/T6UU83TZ/Hoffrage - 2019 - Persönliche Verantwortung und Verantwortungsüberna.pdf},
  journal = {P{\"a}diatrie \& P{\"a}dologie},
  language = {de},
  number = {S1}
}

@article{hornbacher,
  title = {Ethik, {{Ethos}}, {{Ethnos}} - {{Aspekte}} Und {{Probleme}} Interkultureller {{Ethik}}},
  author = {Hornbacher, Annette},
  pages = {432},
  file = {/Users/niels/Zotero/storage/6IN9VSF4/Hornbacher - Ethik, Ethos, Ethnos - Aspekte und Probleme interk.pdf}
}

@misc{ieee,
  title = {{{IEEE}} - {{The}} World's Largest Technical Professional Organization Dedicated to Advancing Technology for the Benefit of Humanity.},
  author = {IEEE},
  abstract = {IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.},
  howpublished = {https://www.ieee.org/}
}

@misc{iso_benefits,
  title = {{{ISO}} - {{Benefits}} of Standards},
  author = {{iso\_benefits}},
  abstract = {Whether you run a business, work for a company or government, or want to know how standards contribute to products and services that you use, you'll find it here.},
  howpublished = {https://www.iso.org/benefits-of-standards.html},
  journal = {ISO},
  language = {en}
}

@misc{iso_sc42,
  title = {{{ISO}} - {{ISO}}/{{IEC JTC}} 1/{{SC}} 42 - {{Artificial}} Intelligence},
  author = {ISO\_SC42},
  howpublished = {https://www.iso.org/committee/6794475/x/catalogue/p/0/u/1/w/0/d/0}
}

@misc{iso22989,
  title = {{{ISO}}/{{IEC CD}} 22989},
  author = {{ISO 22989}},
  abstract = {Artificial intelligence \textemdash{} Concepts and terminology},
  howpublished = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/07/42/74296.html},
  journal = {ISO},
  language = {en}
}

@misc{iso23053,
  title = {{{ISO}}/{{IEC CD}} 23053},
  author = {{iso 23053}},
  abstract = {Framework for Artificial Intelligence (AI) Systems Using Machine Learning (ML)},
  howpublished = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/07/44/74438.html},
  journal = {ISO},
  language = {en}
}

@misc{iso23894,
  title = {{{ISO}}/{{IEC AWI}} 23894},
  author = {{iso 23894}},
  abstract = {Information Technology \textemdash{} Artificial Intelligence \textemdash{} Risk Management},
  howpublished = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/07/73/77304.html},
  journal = {ISO},
  language = {en}
}

@misc{iso24368,
  title = {{{ISO}}/{{IEC AWI TR}} 24368},
  author = {{iso 24368}},
  abstract = {Information technology \textemdash{} Artificial intelligence \textemdash{} Overview of ethical and societal concerns},
  howpublished = {https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/07/85/78507.html},
  journal = {ISO},
  language = {en}
}

@misc{isoabout,
  title = {{{ISO}} - {{About}} Us},
  author = {{iso about}},
  abstract = {ISO brings together global experts to develop International Standards that help solve problems and drive innovation.},
  howpublished = {https://www.iso.org/about-us.html},
  journal = {ISO},
  language = {en}
}

@article{jiang2017,
  title = {Artificial Intelligence in Healthcare: Past, Present and Future},
  shorttitle = {Artificial Intelligence in Healthcare},
  author = {Jiang, Fei and Jiang, Yong and Zhi, Hui and Dong, Yi and Li, Hao and Ma, Sufeng and Wang, Yilong and Dong, Qiang and Shen, Haipeng and Wang, Yongjun},
  year = {2017},
  month = dec,
  volume = {2},
  pages = {230--243},
  issn = {2059-8688, 2059-8696},
  doi = {10.1136/svn-2017-000101},
  abstract = {Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.},
  file = {/Users/niels/Zotero/storage/JX6CHHV8/Jiang et al. - 2017 - Artificial intelligence in healthcare past, prese.pdf},
  journal = {Stroke and Vascular Neurology},
  language = {en},
  number = {4}
}

@article{karampatziakis2019,
  title = {Lessons from {{Contextual Bandit Learning}} in a {{Customer Support Bot}}},
  author = {Karampatziakis, Nikos and Kochman, Sebastian and Huang, Jade and Mineiro, Paul and Osborne, Kathy and Chen, Weizhu},
  year = {2019},
  month = jun,
  abstract = {In this work, we describe practical lessons we have learned from successfully using contextual bandits (CBs) to improve key business metrics of the Microsoft Virtual Agent for customer support. While our current use cases focus on single step reinforcement learning (RL) and mostly in the domain of natural language processing and information retrieval we believe many of our findings are generally applicable. Through this article, we highlight certain issues that RL practitioners may encounter in similar types of applications as well as offer practical solutions to these challenges.},
  archivePrefix = {arXiv},
  eprint = {1905.02219},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/WUTKI8FX/Karampatziakis et al. - 2019 - Lessons from Contextual Bandit Learning in a Custo.pdf},
  journal = {arXiv:1905.02219 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{karnowski2013,
  title = {{Diffusionstheorie}},
  booktitle = {{Handbuch Medienwirkungsforschung}},
  author = {Karnowski, Veronika},
  editor = {Schweiger, Wolfgang and Fahr, Andreas},
  year = {2013},
  pages = {513--528},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-531-18967-3_27},
  abstract = {Die Diffusionstheorie besch{\"a}ftigt sich mit dem Prozess der Verbreitung von Innovationen in einer Gesellschaft. Sie ist in zweierlei Hinsicht relevant f{\"u}r die Kommunikationswissenschaft: zum einen ist der Diffusionsprozess selbst ein ma{\ss}geblich durch massenmediale und interpersonale Kommunikationskan{\"a}le angetriebener Prozess, zum anderen stehen oftmals auch Medieninnovationen selbst im Blickpunkt von Diffusionsstudien. Medienwirkung ist die Diffusion von Innovationen dabei dahingehend, dass massenmediale Kan{\"a}le den Prozess der Verbreitung einer Innovation ma{\ss}geblich mit antreiben. Ebenso ver{\"a}ndern (etwa technische) Innovationen Medienprodukte und -botschaften, ihre Verbreitung und damit auch ihre Wirkungspotenziale. Die Diffusionstheorie konnte seit den 1940er Jahren eine Vielzahl an Faktoren herausarbeiten, die diesen Prozess beeinflussen und ihn auf der Mikro- wie auch auf der Makroebene modellieren. Jedoch gelingt es der Diffusionstheorie \textendash{} trotz einer Vielzahl an empirischen Studien \textendash{} erst in der j{\"u}ngsten Vergangenheit, Kernkritikpunkte sowohl an ihrer theoretischen Konzeption als auch an der methodischen Umsetzung zu {\"u}berwinden.},
  file = {/Users/niels/Zotero/storage/TPRA3Z36/Karnowski - 2013 - Diffusionstheorie.pdf},
  isbn = {978-3-531-18158-5 978-3-531-18967-3},
  language = {de}
}

@book{kersting2019,
  title = {{Wie Maschinen lernen: K{\"u}nstliche Intelligenz verst{\"a}ndlich erkl{\"a}rt}},
  shorttitle = {{Wie Maschinen lernen}},
  editor = {Kersting, Kristian and Lampert, Christoph and Rothkopf, Constantin},
  year = {2019},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-26763-6},
  file = {/Users/niels/Zotero/storage/EZCXZS4U/Kersting et al. - 2019 - Wie Maschinen lernen Künstliche Intelligenz verst.pdf},
  isbn = {978-3-658-26762-9 978-3-658-26763-6},
  language = {de}
}

@article{kober2013,
  title = {Reinforcement Learning in Robotics: {{A}} Survey},
  shorttitle = {Reinforcement Learning in Robotics},
  author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
  year = {2013},
  month = sep,
  volume = {32},
  pages = {1238--1274},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364913495721},
  abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
  file = {/Users/niels/Zotero/storage/HKNJFX3B/Kober et al. - 2013 - Reinforcement learning in robotics A survey.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {11}
}

@article{koch2019,
  title = {Reinforcement {{Learning}} for {{UAV Attitude Control}}},
  author = {Koch, William and Mancuso, Renato and West, Richard and Bestavros, Azer},
  year = {2019},
  month = mar,
  volume = {3},
  pages = {1--21},
  issn = {2378-962X, 2378-9638},
  doi = {10.1145/3301273},
  file = {/Users/niels/Zotero/storage/QM2XC2E8/Koch et al. - 2019 - Reinforcement Learning for UAV Attitude Control.pdf},
  journal = {ACM Transactions on Cyber-Physical Systems},
  language = {en},
  number = {2}
}

@book{koelbl2018,
  title = {{Stichw{\"o}rter zur Kulturpsychologie}},
  editor = {K{\"o}lbl, Carlos and Sieben, Anna},
  year = {2018},
  publisher = {{Psychosozial-Verlag}},
  doi = {10.30820/9783837974522},
  file = {/Users/niels/Zotero/storage/A26F5DYW/Kölbl und Sieben - 2018 - Stichwörter zur Kulturpsychologie.pdf},
  isbn = {978-3-8379-7452-2},
  language = {de}
}

@article{koopman2016,
  title = {Challenges in {{Autonomous Vehicle Testing}} and {{Validation}}},
  author = {Koopman, Philip and Wagner, Michael},
  year = {2016},
  month = apr,
  volume = {4},
  pages = {15--24},
  issn = {2327-5634},
  doi = {10.4271/2016-01-0128},
  abstract = {Software testing is all too often simply a bug hunt rather than a wellconsidered exercise in ensuring quality. A more methodical approach than a simple cycle of system-level test-fail-patch-test will be required to deploy safe autonomous vehicles at scale. The ISO 26262 development V process sets up a framework that ties each type of testing to a corresponding design or requirement document, but presents challenges when adapted to deal with the sorts of novel testing problems that face autonomous vehicles. This paper identifies five major challenge areas in testing according to the V model for autonomous vehicles: driver out of the loop, complex requirements, non-deterministic algorithms, inductive learning algorithms, and failoperational systems. General solution approaches that seem promising across these different challenge areas include: phased deployment using successively relaxed operational scenarios, use of a monitor/actuator pair architecture to separate the most complex autonomy functions from simpler safety functions, and fault injection as a way to perform more efficient edge case testing. While significant challenges remain in safety-certifying the type of algorithms that provide high-level autonomy themselves, it seems within reach to instead architect the system and its accompanying design process to be able to employ existing software safety approaches.},
  file = {/Users/niels/Zotero/storage/SHZEW2AC/Koopman und Wagner - 2016 - Challenges in Autonomous Vehicle Testing and Valid.pdf},
  journal = {SAE International Journal of Transportation Safety},
  language = {en},
  number = {1}
}

@book{kramer2009,
  title = {{Computational intelligence: eine einf{\"u}hrung}},
  shorttitle = {{Computational intelligence}},
  author = {Kramer, Oliver},
  year = {2009},
  publisher = {{Springer}},
  address = {{Dordrecht ; New York}},
  file = {/Users/niels/Zotero/storage/KIZJCBB9/Kramer - 2009 - Computational intelligence eine einführung 2.pdf},
  isbn = {978-3-540-79738-8 978-3-540-79739-5},
  keywords = {Computational intelligence},
  language = {de},
  lccn = {Q342 .K73 2009},
  series = {{Informatik im fokus}}
}

@article{krause,
  title = {Safe {{Exploration}} in {{Reinforcement Learning}}: {{Theory}} and {{Applications}} in {{Robotics}}},
  author = {Krause, Dr Andreas and Schollig, Dr Angela P and Morari, Dr Manfred},
  pages = {207},
  file = {/Users/niels/Zotero/storage/2DMY3ZF2/Krause et al. - Felix Mick Finn Berkenkamp M.Sc. ETH, ETH Zurich b.pdf},
  language = {en}
}

@book{lefrancois1986,
  title = {{Psychologie des Lernens}},
  author = {Lefrancois, Guy R.},
  editor = {Leppmann, Peter K. and Angermeier, Wilhelm F. and Thiek{\"o}tter, Thomas J.},
  year = {1986},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-09577-5},
  file = {/Users/niels/Zotero/storage/CDDU3F86/Lefrancois - 1986 - Psychologie des Lernens.pdf},
  isbn = {978-3-540-16192-9 978-3-662-09577-5},
  language = {de},
  series = {{Springer-Lehrbuch}}
}

@article{li2018,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Li, Yuxi},
  year = {2018},
  month = nov,
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model and planning, exploration, and knowledge. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multiagent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, business management, finance, healthcare, education, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  archivePrefix = {arXiv},
  eprint = {1701.07274},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/6WLEZMUY/Li - 2018 - Deep Reinforcement Learning An Overview.pdf},
  journal = {arXiv:1701.07274 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{liu2017,
  title = {Deep {{Reinforcement Learning}} for {{Dynamic Treatment Regimes}} on {{Medical Registry Data}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Healthcare Informatics}} ({{ICHI}})},
  author = {Liu, Ying and Logan, Brent and Liu, Ning and Xu, Zhiyuan and Tang, Jian and Wang, Yangzhi},
  year = {2017},
  month = aug,
  pages = {380--385},
  publisher = {{IEEE}},
  address = {{Park City, UT, USA}},
  doi = {10.1109/ICHI.2017.45},
  abstract = {In this paper, we propose the first deep reinforcement learning framework to estimate the optimal Dynamic Treatment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.},
  file = {/Users/niels/Zotero/storage/TCBUB5CP/Liu et al. - 2017 - Deep Reinforcement Learning for Dynamic Treatment .pdf},
  isbn = {978-1-5090-4881-6},
  language = {en}
}

@book{mcnaughton1988,
  title = {Moral Vision: An Introduction to Ethics},
  shorttitle = {Moral Vision},
  author = {McNaughton, David},
  year = {1988},
  publisher = {{B. Blackwell}},
  address = {{Oxford, UK ; New York, NY}},
  file = {/Users/niels/Zotero/storage/A2TDZSR6/McNaughton - 1988 - Moral vision an introduction to ethics.pdf},
  isbn = {978-0-631-15408-2 978-0-631-15945-2},
  keywords = {Ethics},
  language = {en},
  lccn = {BJ1012 .M42 1988}
}

@book{mey2010,
  title = {{Handbuch qualitative Forschung in der Psychologie}},
  editor = {Mey, G{\"u}nter and Mruck, Katja},
  year = {2010},
  edition = {1. Aufl},
  publisher = {{VS Verlag f{\"u}r Sozialwissenschaften}},
  address = {{Wiesbaden}},
  file = {/Users/niels/Zotero/storage/U37QQDE3/Mey and Mruck - 2010 - Handbuch qualitative Forschung in der Psychologie.pdf},
  isbn = {978-3-531-16726-8},
  keywords = {Handbooks; manuals; etc,Methodology,Psychology,Qualitative research},
  language = {de},
  lccn = {BF76.5 .H36 2010}
}

@techreport{mit2019,
  title = {Asia`s {{AI}} Agenda - {{The}} Ethics of {{AI}}},
  author = {MIT, Technology Review Insights},
  year = {2019},
  institution = {{MIT}},
  file = {/Users/niels/Zotero/storage/3TNJL6FG/asiaaiethics.pdf}
}

@article{moll2003,
  title = {Morals and the Human Brain: A Working Model:},
  shorttitle = {Morals and the Human Brain},
  author = {Moll, Jorge and {de Oliveira-Souza}, Ricardo and Eslinger, Paul J.},
  year = {2003},
  month = mar,
  pages = {299--305},
  issn = {0959-4965},
  doi = {10.1097/00001756-200303030-00001},
  file = {/Users/niels/Zotero/storage/CKUJN6TD/Moll et al. - 2003 - Morals and the human brain a working model.pdf},
  journal = {NeuroReport},
  language = {en}
}

@article{moor2006,
  title = {The {{Nature}}, {{Importance}}, and {{Difficulty}} of {{Machine Ethics}}},
  author = {Moor, James H},
  year = {2006},
  pages = {4},
  file = {/Users/niels/Zotero/storage/6YLZG98L/Moor - 2006 - The Nature, Importance, and Difficulty of Machine .pdf},
  journal = {IEEE INTELLIGENT SYSTEMS},
  language = {en}
}

@misc{mozur2018,
  title = {Inside {{China}}'s {{Dystopian Dreams}}: {{A}}.{{I}}., {{Shame}} and {{Lots}} of {{Cameras}}},
  author = {Mozur, Paul},
  year = {2018},
  journal = {NY Times},
  language = {en}
}

@incollection{muehlhauser2012,
  title = {The {{Singularity}} and {{Machine Ethics}}},
  booktitle = {Singularity {{Hypotheses}}},
  author = {Muehlhauser, Luke and Helm, Louie},
  editor = {Eden, Amnon H. and Moor, James H. and S{\o}raker, Johnny H. and Steinhart, Eric},
  year = {2012},
  pages = {101--126},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32560-1_6},
  file = {/Users/niels/Zotero/storage/8TE35JHZ/Muehlhauser and Helm - 2012 - The Singularity and Machine Ethics.pdf},
  isbn = {978-3-642-32559-5 978-3-642-32560-1},
  language = {en}
}

@article{mulcahylevy2018,
  title = {Linking Brain Tumors and Epileptic Seizures},
  author = {Mulcahy Levy, Jean M. and McMahon, Martin},
  year = {2018},
  month = nov,
  volume = {24},
  pages = {1638--1639},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-018-0249-6},
  file = {/Users/niels/Zotero/storage/5P6HENG2/Mulcahy Levy und McMahon - 2018 - Linking brain tumors and epileptic seizures.pdf},
  journal = {Nature Medicine},
  language = {en},
  number = {11}
}

@inproceedings{nagendra2017,
  title = {Comparison of Reinforcement Learning Algorithms Applied to the Cart-Pole Problem},
  booktitle = {2017 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  author = {Nagendra, Savinay and Podila, Nikhil and Ugarakhod, Rashmi and George, Koshy},
  year = {2017},
  month = sep,
  pages = {26--32},
  publisher = {{IEEE}},
  address = {{Udupi}},
  doi = {10.1109/ICACCI.2017.8125811},
  abstract = {Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cartpole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation are compared in this context with the standard LQR solution. Further, we propose a novel approach to integrate RL and swing-up controllers.},
  file = {/Users/niels/Zotero/storage/6XGXGIBW/Nagendra et al. - 2017 - Comparison of reinforcement learning algorithms ap.pdf},
  isbn = {978-1-5090-6367-3},
  language = {en}
}

@article{nair2018,
  title = {Overcoming {{Exploration}} in {{Reinforcement Learning}} with {{Demonstrations}}},
  author = {Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2018},
  month = feb,
  abstract = {Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.},
  archivePrefix = {arXiv},
  eprint = {1709.10089},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/3VEH7HXT/Nair et al. - 2018 - Overcoming Exploration in Reinforcement Learning w.pdf},
  journal = {arXiv:1709.10089 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  language = {en},
  primaryClass = {cs}
}

@book{neuhauser2017,
  title = {Military {{Interventions}}: {{Considerations}} from {{Philosophy}} and {{Political Science}}},
  shorttitle = {Military {{Interventions}}},
  editor = {Neuh{\"a}user, Christian and Schuck, Christoph},
  year = {2017},
  publisher = {{Nomos Verlagsgesellschaft mbH \& Co. KG}},
  doi = {10.5771/9783845278629},
  file = {/Users/niels/Zotero/storage/EYIMKN53/Neuhäuser und Schuck - 2017 - Military Interventions Considerations from Philos.pdf},
  isbn = {978-3-8452-7862-9},
  language = {en}
}

@book{nietsch-hach2016,
  ids = {nietsch-hach2016a,nietsch-hach2016b,nietsch-hach2016c},
  title = {{Ethisches Verhalten in der modernen Wirtschaftswelt}},
  author = {{Nietsch-Hach}, Cornelia},
  year = {2016},
  publisher = {{UVK Verlagsgesellschaft mbH}},
  doi = {10.24053/9783739801285},
  file = {/Users/niels/Zotero/storage/AQ6F8UIB/Nietsch-Hach - 2016 - Ethisches Verhalten in der modernen Wirtschaftswel.pdf;/Users/niels/Zotero/storage/EINNYVAL/Nietsch-Hach - 2016 - Ethisches Verhalten in der modernen Wirtschaftswel.pdf;/Users/niels/Zotero/storage/TA4VUE5I/Nietsch-Hach - 2016 - Ethisches Verhalten in der modernen Wirtschaftswel.pdf;/Users/niels/Zotero/storage/XGSURKBP/Nietsch-Hach - 2016 - Ethisches Verhalten in der modernen Wirtschaftswel.pdf},
  isbn = {978-3-7398-0128-5},
  language = {de}
}

@book{nietsch-hachEthischesVerhaltenModernen2016a,
  title = {{Ethisches Verhalten in der modernen Wirtschaftswelt}},
  author = {{Nietsch-Hach}, Cornelia},
  year = {2016},
  publisher = {{UVK Verlagsgesellschaft mbH}},
  doi = {10.24053/9783739801285},
  isbn = {978-3-7398-0128-5},
  language = {de}
}

@book{nietsch-hachEthischesVerhaltenModernen2016b,
  title = {{Ethisches Verhalten in der modernen Wirtschaftswelt}},
  author = {{Nietsch-Hach}, Cornelia},
  year = {2016},
  publisher = {{UVK Verlagsgesellschaft mbH}},
  doi = {10.24053/9783739801285},
  isbn = {978-3-7398-0128-5},
  language = {de}
}

@book{nietsch-hachEthischesVerhaltenModernen2016c,
  title = {{Ethisches Verhalten in der modernen Wirtschaftswelt}},
  author = {{Nietsch-Hach}, Cornelia},
  year = {2016},
  publisher = {{UVK Verlagsgesellschaft mbH}},
  doi = {10.24053/9783739801285},
  isbn = {978-3-7398-0128-5},
  language = {de}
}

@book{nietsch-hachEthischesVerhaltenModernen2016d,
  title = {{Ethisches Verhalten in der modernen Wirtschaftswelt}},
  author = {{Nietsch-Hach}, Cornelia},
  year = {2016},
  publisher = {{UVK Verlagsgesellschaft mbH}},
  doi = {10.24053/9783739801285},
  isbn = {978-3-7398-0128-5},
  language = {de}
}

@techreport{nitiaayog2018,
  title = {National Strategy for Artificial Intelligence \#{{AIFORALL}}},
  author = {NITI Aayog},
  year = {2018},
  file = {/Users/niels/Zotero/storage/VBBLSRI5/NationalStrategy-for-AI-Discussion-Paper.pdf}
}

@inproceedings{niu2009,
  title = {Application of {{Reinforcement Learning}} in {{Autonomous Navigation}} for {{Virtual Vehicles}}},
  booktitle = {2009 {{Ninth International Conference}} on {{Hybrid Intelligent Systems}}},
  author = {Niu, Lianqiang and Li, Ling},
  year = {2009},
  pages = {30--32},
  publisher = {{IEEE}},
  address = {{Shenyang, China}},
  doi = {10.1109/HIS.2009.118},
  abstract = {To solve the basic problem in the autonomous navigation for the virtual vehicles, the application of reinforcement learning is discussed, combined with artificial neural network, a model is proposed to realize the autonomous navigation for virtual vehicles. The autonomous navigation for the virtual vehicles is accomplished by reinforcement learning, the generalization problem for the action selection is solved by neural network, the model is tested by virtual environment, and the experiments show that the model is available and feasible.},
  file = {/Users/niels/Zotero/storage/5BSP3KS5/Niu and Li - 2009 - Application of Reinforcement Learning in Autonomou.pdf},
  isbn = {978-0-7695-3745-0},
  language = {en}
}

@article{noothigattu,
  title = {Teaching {{AI Agents Ethical Values Using Reinforcement Learning}} and {{Policy Orchestration}}},
  author = {Noothigattu, Ritesh and Bouneffouf, Djallel and Mattei, Nicholas and Chandra, Rachita and Madan, Piyush and Varshney, Kush R and Campbell, Murray and Singh, Moninder and Rossi, Francesca},
  pages = {5},
  abstract = {Autonomous cyber-physical agents play an increasingly large role in our lives. To ensure that they behave in ways aligned with the values of society, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations and reinforcement learning to learn to maximize environmental rewards. A contextual banditbased orchestrator then picks between the two policies: constraint-based and environment rewardbased. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward-maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using PacMan and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.},
  file = {/Users/niels/Zotero/storage/IPTLI9QY/Noothigattu et al. - Teaching AI Agents Ethical Values Using Reinforcem.pdf},
  language = {en}
}

@incollection{olufowobi2017,
  title = {Data {{Provenance Model}} for {{Internet}} of {{Things}} ({{IoT}}) {{Systems}}},
  booktitle = {Service-{{Oriented Computing}} \textendash{} {{ICSOC}} 2016 {{Workshops}}},
  author = {Olufowobi, Habeeb and Engel, Robert and Baracaldo, Nathalie and Bathen, Luis Angel D. and Tata, Samir and Ludwig, Heiko},
  editor = {Drira, Khalil and Wang, Hongbing and Yu, Qi and Wang, Yan and Yan, Yuhong and Charoy, Fran{\c c}ois and Mendling, Jan and Mohamed, Mohamed and Wang, Zhongjie and Bhiri, Sami},
  year = {2017},
  volume = {10380},
  pages = {85--91},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-68136-8_8},
  abstract = {Internet of Things (IoT) systems and applications are increasingly deployed for critical use cases and therefore exhibit an increasing need for dependability. Data provenance deals with the recording, management and retrieval of information about the origin and history of data. We propose that the introduction of data provenance concepts into the IoT domain can help create dependable and trustworthy IoT systems by recording the lineage of data from basic sensor readings up to complex derived information created by software agents. In this paper, we present a data provenance model for IoT systems that is geared towards providing a generic mechanism for assuring the correctness and integrity of IoT applications and thereby reinforcing their trustworthiness and dependability for critical use cases.},
  file = {/Users/niels/Zotero/storage/ETW4N5CY/Olufowobi et al. - 2017 - Data Provenance Model for Internet of Things (IoT).pdf},
  isbn = {978-3-319-68135-1 978-3-319-68136-8},
  language = {en}
}

@misc{openai,
  title = {{{OpenAI}}},
  author = {OpenAI}
}

@article{packer2019,
  title = {Assessing {{Generalization}} in {{Deep Reinforcement Learning}}},
  author = {Packer, Charles and Gao, Katelyn and Kos, Jernej and Kr{\"a}henb{\"u}hl, Philipp and Koltun, Vladlen and Song, Dawn},
  year = {2019},
  month = mar,
  abstract = {Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but agents often fail to generalize beyond the environment they were trained in. As a result, deep RL algorithms that promote generalization are receiving increasing attention. However, works in this area use a wide variety of tasks and experimental setups for evaluation. The literature lacks a controlled assessment of the merits of different generalization schemes. Our aim is to catalyze communitywide progress on generalization in deep RL. To this end, we present a benchmark and experimental protocol, and conduct a systematic empirical study. Our framework contains a diverse set of environments, our methodology covers both indistribution and out-of-distribution generalization, and our evaluation includes deep RL algorithms that specifically tackle generalization. Our key finding is that ``vanilla'' deep RL algorithms generalize better than specialized schemes that were proposed specifically to tackle generalization.},
  archivePrefix = {arXiv},
  eprint = {1810.12282},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/4PWYRWBR/Packer et al. - 2019 - Assessing Generalization in Deep Reinforcement Lea.pdf},
  journal = {arXiv:1810.12282 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pan2017,
  title = {Virtual to {{Real Reinforcement Learning}} for {{Autonomous Driving}}},
  author = {Pan, Xinlei and You, Yurong and Wang, Ziyan and Lu, Cewu},
  year = {2017},
  month = sep,
  abstract = {Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.},
  archivePrefix = {arXiv},
  eprint = {1704.03952},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/8WA2LD5E/Pan et al. - 2017 - Virtual to Real Reinforcement Learning for Autonom.pdf},
  journal = {arXiv:1704.03952 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{panch2019,
  title = {The ``Inconvenient Truth'' about {{AI}} in Healthcare},
  author = {Panch, Trishan and Mattie, Heather and Celi, Leo Anthony},
  year = {2019},
  month = dec,
  volume = {2},
  pages = {77},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0155-4},
  file = {/Users/niels/Zotero/storage/XUYESXFE/Panch et al. - 2019 - The “inconvenient truth” about AI in healthcare.pdf},
  journal = {npj Digital Medicine},
  language = {en},
  number = {1}
}

@article{peters,
  title = {Reinforcement {{Learning}} for {{Humanoid Robotics}}},
  author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
  pages = {20},
  abstract = {Reinforcement learning offers one of the most general framework to take traditional robotics towards true autonomy and versatility. However, applying reinforcement learning to high dimensional movement systems like humanoid robots remains an unsolved problem. In this paper, we discuss different approaches of reinforcement learning in terms of their applicability in humanoid robotics. Methods can be coarsely classified into three different categories, i.e., greedy methods, `vanilla' policy gradient methods, and natural gradient methods. We discuss that greedy methods are not likely to scale into the domain humanoid robotics as they are problematic when used with function approximation. `Vanilla' policy gradient methods on the other hand have been successfully applied on real-world robots including at least one humanoid robot [3]. We demonstrate that these methods can be significantly improved using the natural policy gradient instead of the regular policy gradient. A derivation of the natural policy gradient is provided, proving that the average policy gradient of Kakade [10] is indeed the true natural gradient. A general algorithm for estimating the natural gradient, the Natural Actor-Critic algorithm, is introduced. This algorithm converges to the nearest local minimum of the cost function with respect to the Fisher information metric under suitable conditions. The algorithm outperforms non-natural policy gradients by far in a cart-pole balancing evaluation, and for learning nonlinear dynamic motor primitives for humanoid robot control. It offers a promising route for the development of reinforcement learning for truly high-dimensionally continuous state-action systems.},
  file = {/Users/niels/Zotero/storage/UJJI2NJ8/Peters et al. - Reinforcement Learning for Humanoid Robotics.pdf},
  language = {en}
}

@misc{pls,
  title = {Plattform {{Lernende Systeme}} - {{PLS}}},
  author = {{pls}},
  howpublished = {https://www.plattform-lernende-systeme.de/startseite.html}
}

@misc{plsa,
  title = {{{AG}} 3 - {{PLS}}},
  author = {{pls}},
  file = {/Users/niels/Zotero/storage/CJCMMPJD/ag-3.html},
  howpublished = {https://www.plattform-lernende-systeme.de/ag-3.html}
}

@misc{prism,
  title = {{{PRISM}} - {{Probabilistic Symbolic Model Checker}}},
  author = {PRISM},
  howpublished = {https://www.prismmodelchecker.org/}
}

@book{rath2019,
  title = {{Maschinenethik: Normative Grenzen autonomer Systeme}},
  shorttitle = {{Maschinenethik}},
  editor = {Rath, Matthias and Krotz, Friedrich and Karmasin, Matthias},
  year = {2019},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-21083-0},
  file = {/Users/niels/Zotero/storage/ZGJBHTEK/Rath et al. - 2019 - Maschinenethik Normative Grenzen autonomer System.pdf},
  isbn = {978-3-658-21082-3 978-3-658-21083-0},
  language = {de},
  series = {{Ethik in mediatisierten Welten}}
}

@article{ribeiro2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archivePrefix = {arXiv},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/K4JN2J59/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf},
  journal = {arXiv:1602.04938 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{richter2019,
  title = {Open-{{Sourced Reinforcement Learning Environments}} for {{Surgical Robotics}}},
  author = {Richter, Florian and Orosco, Ryan K. and Yip, Michael C.},
  year = {2019},
  month = mar,
  abstract = {Reinforcement Learning (RL) is a machine learning framework for artificially intelligent systems to solve a variety of complex problems. Recent years has seen a surge of successes solving challenging games and smaller domain problems, including simple though non-specific robotic manipulation and grasping tasks. Rapid successes in RL have come in part due to the strong collaborative effort by the RL community to work on common, open-sourced environment simulators such as OpenAI's Gym that allow for expedited development and valid comparisons between different, state-of-art strategies. In this paper, we aim to bridge the RL and the surgical robotics communities by presenting the first open-sourced reinforcement learning environments for surgical robotics, called dVRL3. Through the proposed RL environment, which are functionally equivalent to Gym, we show that it is easy to prototype and implement state-of-art RL algorithms on surgical robotics problems that aim to introduce autonomous robotic precision and accuracy to assisting, collaborative, or repetitive tasks during surgery. Learned policies are furthermore successfully transferable to a real robot. Finally, combining dVRL with the over 40+ international network of da Vinci Surgical Research Kits in active use at academic institutions, we see dVRL as enabling the broad surgical robotics community to fully leverage the newest strategies in reinforcement learning, and for reinforcement learning scientists with no knowledge of surgical robotics to test and develop new algorithms that can solve the real-world, high-impact challenges in autonomous surgery.},
  archivePrefix = {arXiv},
  eprint = {1903.02090},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/SSHW2HXU/Richter et al. - 2019 - Open-Sourced Reinforcement Learning Environments f.pdf},
  journal = {arXiv:1903.02090 [cs]},
  keywords = {Computer Science - Robotics},
  language = {en},
  primaryClass = {cs}
}

@article{rogers2016,
  title = {Bridging the 21st {{Century Digital Divide}}},
  author = {Rogers, Sylvia E.},
  year = {2016},
  month = may,
  volume = {60},
  pages = {197--199},
  issn = {8756-3894, 1559-7075},
  doi = {10.1007/s11528-016-0057-0},
  file = {/Users/niels/Zotero/storage/L6B24GYK/Rogers - 2016 - Bridging the 21st Century Digital Divide.pdf},
  journal = {TechTrends},
  language = {en},
  number = {3}
}

@misc{sac,
  title = {国家标准化管理委员会},
  author = {SAC},
  abstract = {Standardisierungsinstitution Chinas},
  howpublished = {http://www.sac.gov.cn/}
}

@article{sallab2017,
  title = {Deep {{Reinforcement Learning}} Framework for {{Autonomous Driving}}},
  author = {Sallab, AhmadEL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil},
  year = {2017},
  month = jan,
  volume = {2017},
  pages = {70--76},
  issn = {2470-1173},
  doi = {10.2352/ISSN.2470-1173.2017.19.AVM-023},
  abstract = {Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes. Despite its perceived utility, it has not yet been successfully applied in automotive applications. Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning. This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks. As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework. It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios. It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware. The framework was tested in an open source 3D car racing simulator called TORCS. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles.},
  file = {/Users/niels/Zotero/storage/FMF8DZSR/Sallab et al. - 2017 - Deep Reinforcement Learning framework for Autonomo.pdf},
  journal = {Electronic Imaging},
  language = {en},
  number = {19}
}

@article{sanchez-lopez2016,
  title = {A {{Reliable Open}}-{{Source System Architecture}} for the {{Fast Designing}} and {{Prototyping}} of {{Autonomous Multi}}-{{UAV Systems}}: {{Simulation}} and {{Experimentation}}},
  shorttitle = {A {{Reliable Open}}-{{Source System Architecture}} for the {{Fast Designing}} and {{Prototyping}} of {{Autonomous Multi}}-{{UAV Systems}}},
  author = {{Sanchez-Lopez}, Jose Luis and Pestana, Jes{\'u}s and {de la Puente}, Paloma and Campoy, Pascual},
  year = {2016},
  month = dec,
  volume = {84},
  pages = {779--797},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-015-0288-x},
  file = {/Users/niels/Zotero/storage/F7E3IJRK/Sanchez-Lopez et al. - 2016 - A Reliable Open-Source System Architecture for the.pdf},
  journal = {Journal of Intelligent \& Robotic Systems},
  language = {en},
  number = {1-4}
}

@book{schicha2010,
  title = {Handbuch {{Medienethik}}},
  editor = {Schicha, Christian and Brosda, Carsten},
  year = {2010},
  edition = {1. Aufl},
  publisher = {{VS Verlag f{\"u}r Sozialwissenschaften}},
  address = {{Wiesbaden}},
  file = {/Users/niels/Zotero/storage/3FI6QRCD/Schicha und Brosda - 2010 - Handbuch Medienethik.pdf},
  isbn = {978-3-531-15822-8},
  keywords = {Cross-cultural studies,Ethik,Germany,Journalistic ethics,Lehrbuch,Mass media,Massenmedien,Moral and ethical aspects},
  lccn = {PN4756 .H366 2010},
  note = {OCLC: ocn606927422}
}

@book{schultz2006,
  title = {Contemporary Issues in Ethics and Information Technology},
  author = {Schultz, Robert A.},
  year = {2006},
  publisher = {{IRM Press}},
  address = {{Hershey, PA}},
  abstract = {"This book uses general ethical principles as a basis for solutions to solving ethical problems in information technology use within organizations"--Provided by publisher},
  file = {/Users/niels/Zotero/storage/EP7SP3PV/Schultz - 2006 - Contemporary issues in ethics and information tech.pdf},
  isbn = {978-1-59140-779-9 978-1-59140-780-5 978-1-59140-781-2},
  keywords = {Ethics,Information technology,Moral and ethical aspects,Technology},
  language = {en},
  lccn = {BJ995 .S38 2006}
}

@article{schulzke2013,
  title = {Autonomous {{Weapons}} and {{Distributed Responsibility}}},
  author = {Schulzke, Marcus},
  year = {2013},
  month = jun,
  volume = {26},
  pages = {203--219},
  issn = {2210-5433, 2210-5441},
  doi = {10.1007/s13347-012-0089-0},
  abstract = {The possibility that autonomous weapons will be deployed on the battlefields of the future raises the challenge of determining who can be held responsible for how these weapons act. Robert Sparrow has argued that it would be impossible to attribute responsibility for autonomous robots' actions to their creators, their commanders, or the robots themselves. This essay reaches a much different conclusion. It argues that the problem of determining responsibility for autonomous robots can be solved by addressing it within the context of the military chain of command. The military hierarchy is a system of distributing responsibility between decision makers on different levels and constraining autonomy. If autonomous weapons are employed as agents operating within this system, then responsibility for their actions can be attributed to their creators and their civilian and military superiors.},
  file = {/Users/niels/Zotero/storage/PMHIE7N8/Schulzke - 2013 - Autonomous Weapons and Distributed Responsibility 2.pdf},
  journal = {Philosophy \& Technology},
  language = {en},
  number = {2}
}

@inproceedings{sengupta2018,
  title = {Techniques to {{Elimenate Human Bias}} in {{Machine Learning}}},
  booktitle = {2018 {{International Conference}} on {{System Modeling}} \& {{Advancement}} in {{Research Trends}} ({{SMART}})},
  author = {Sengupta, Eishvak and Garg, Dhruv and Choudhury, Tanupriya and Aggarwal, Archit},
  year = {2018},
  month = nov,
  pages = {226--230},
  publisher = {{IEEE}},
  address = {{Moradabad, India}},
  doi = {10.1109/SYSMART.2018.8746946},
  abstract = {In an era where human lives have certain dependence on artificial intelligence and machine learning, it is essential for them to make unbiased and accurate predictions. This paper addresses the issue of the inclusion of a human bias in a machine learning algorithm and how it goes to produce skewed results. It goes through the prominent types of human biases and real life incidents where the inclusion of a human bias has had a negative impact. This paper provides a comprehensive review of the methods that can be incorporated to eliminate a human bias focusing on the use of machine ethics making mention of community groups working towards the same.},
  file = {/Users/niels/Zotero/storage/N2UWAABA/Sengupta et al. - 2018 - Techniques to Elimenate Human Bias in Machine Lear.pdf},
  isbn = {978-1-5386-6369-1},
  language = {en}
}

@article{sequeira2019,
  title = {Interestingness {{Elements}} for {{Explainable Reinforcement Learning}}: {{Understanding Agents}}' {{Capabilities}} and {{Limitations}}},
  shorttitle = {Interestingness {{Elements}} for {{Explainable Reinforcement Learning}}},
  author = {Sequeira, Pedro and Gervasio, Melinda},
  year = {2019},
  month = dec,
  abstract = {We propose an explainable reinforcement learning (XRL) framework that analyzes an agent's history of interaction with the environment to extract interestingness elements that help explain its behavior. The framework relies on data readily available from standard RL algorithms, augmented with data that can easily be collected by the agent while learning. We describe how to create visual explanations of an agent's behavior in the form of short video-clips highlighting key interaction moments, based on the proposed elements. We also report on a user study where we evaluated the ability of humans in correctly perceiving the aptitude of agents with different characteristics, including their capabilities and limitations, given explanations automatically generated by our framework. The results show that the diversity of aspects captured by the different interestingness elements is crucial to help humans correctly identify the agents' aptitude in the task, and determine when they might need adjustments to improve their performance.},
  archivePrefix = {arXiv},
  eprint = {1912.09007},
  eprinttype = {arxiv},
  file = {/Users/niels/Zotero/storage/TYFH8ZD8/Sequeira und Gervasio - 2019 - Interestingness Elements for Explainable Reinforce.pdf},
  journal = {arXiv:1912.09007 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sharkey2014,
  title = {Robots and Human Dignity: A Consideration of the Effects of Robot Care on the Dignity of Older People},
  shorttitle = {Robots and Human Dignity},
  author = {Sharkey, Amanda},
  year = {2014},
  month = mar,
  volume = {16},
  pages = {63--75},
  issn = {1388-1957, 1572-8439},
  doi = {10.1007/s10676-014-9338-5},
  abstract = {This paper explores the relationship between dignity and robot care for older people. It highlights the disquiet that is often expressed about failures to maintain the dignity of vulnerable older people, but points out some of the contradictory uses of the word `dignity'. Certain authors have resolved these contradictions by identifying different senses of dignity; contrasting the inviolable dignity inherent in human life to other forms of dignity which can be present to varying degrees. The capability approach (CA) is introduced as a different but tangible account of what it means to live a life worthy of human dignity. It is used here as a framework for the assessment of the possible effects of eldercare robots on human dignity. The CA enables the identification of circumstances in which robots could enhance dignity by expanding the set of capabilities that are accessible to frail older people. At the same time, it is also possible within its framework to identify ways in which robots could have a negative impact, by impeding the access of older people to essential capabilities. It is concluded that the CA has some advantages over other accounts of dignity, but that further work and empirical study is needed in order to adapt it to the particular circumstances and concerns of those in the latter part of their lives.},
  file = {/Users/niels/Zotero/storage/EG5YJGSN/Sharkey - 2014 - Robots and human dignity a consideration of the e.pdf},
  journal = {Ethics and Information Technology},
  language = {en},
  number = {1}
}

@article{sharkey2019,
  title = {Autonomous Weapons Systems, Killer Robots and Human Dignity},
  author = {Sharkey, Amanda},
  year = {2019},
  month = jun,
  volume = {21},
  pages = {75--87},
  issn = {1388-1957, 1572-8439},
  doi = {10.1007/s10676-018-9494-0},
  abstract = {One of the several reasons given in calls for the prohibition of autonomous weapons systems (AWS) is that they are against human dignity (Asaro in Int Rev Red Cross 94(886):687\textendash{}709, 2012; Docherty in Shaking the foundations: the human rights implications of killer robots, Human Rights Watch, New York, 2014; Heyns in S Afr J Hum Rights 33(1):46\textendash{}71, 2017; Ulgen in Human dignity in an age of autonomous weapons: are we in danger of losing an `elementary consideration of humanity'? 2016). However there have been criticisms of the reliance on human dignity in arguments against AWS (Birnbacher in Autonomous weapons systems: law, ethics, policy, Cambridge University Press, Cambridge, 2016; Pop in Autonomous weapons systems: a threat to human dignity? 2018; Saxton in (Un)dignified killer robots? The problem with the human dignity argument, 2016). This paper critically examines the relationship between human dignity and AWS. Three main types of objection to AWS are identified; (i) arguments based on technology and the ability of AWS to conform to international humanitarian law; (ii) deontological arguments based on the need for human judgement and meaningful human control, including arguments based on human dignity; (iii) consequentialist reasons about their effects on global stability and the likelihood of going to war. An account is provided of the claims made about human dignity and AWS, of the criticisms of these claims, and of the several meanings of `dignity'. It is concluded that although there are several ways in which AWS can be said to be against human dignity, they are not unique in this respect. There are other weapons, and other technologies, that also compromise human dignity. Given this, and the ambiguities inherent in the concept, it is wiser to draw on several types of objections in arguments against AWS, and not to rely exclusively on human dignity.},
  file = {/Users/niels/Zotero/storage/DWPCR9K6/Sharkey - 2019 - Autonomous weapons systems, killer robots and huma.pdf},
  journal = {Ethics and Information Technology},
  language = {en},
  number = {2}
}

@techreport{singh2005,
  title = {Intrinsically {{Motivated Reinforcement Learning}}:},
  shorttitle = {Intrinsically {{Motivated Reinforcement Learning}}},
  author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
  year = {2005},
  month = jan,
  address = {{Fort Belvoir, VA}},
  institution = {{Defense Technical Information Center}},
  doi = {10.21236/ADA440280},
  abstract = {Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.},
  file = {/Users/niels/Zotero/storage/2IU6JHH2/Singh et al. - 2005 - Intrinsically Motivated Reinforcement Learning.pdf},
  language = {en}
}

@techreport{smuha,
  title = {{Ethik-Leitlinien f{\"u}r eine vertrauensw{\"u}rdige KI}},
  author = {Smuha, Nathalie},
  pages = {51},
  institution = {{Unabh{\"a}ngige Hochrangige Expertengruppe f{\"u}r K{\"u}nstliche Intelligenz}},
  file = {/Users/niels/Zotero/storage/G8I2KQJQ/EthicsguidelinesfortrustworthyAI-DEpdf.pdf},
  language = {de}
}

@article{sparrow2007,
  title = {Killer {{Robots}}},
  author = {Sparrow, Robert},
  year = {2007},
  month = feb,
  volume = {24},
  pages = {62--77},
  issn = {0264-3758, 1468-5930},
  doi = {10.1111/j.1468-5930.2007.00346.x},
  file = {/Users/niels/Zotero/storage/E4VBLQU4/Sparrow - 2007 - Killer Robots.pdf},
  journal = {Journal of Applied Philosophy},
  language = {en},
  number = {1}
}

@book{sutton2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  file = {/Users/niels/Zotero/storage/SGLP2ERP/Sutton und Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@book{tannsjo2008,
  title = {Understanding Ethics: An Introduction to Moral Theory},
  shorttitle = {Understanding Ethics},
  author = {T{\"a}nnsj{\"o}, Torbj{\"o}rn},
  year = {2008},
  edition = {2nd ed},
  publisher = {{Edinburgh Univ. Press}},
  address = {{Edinburgh}},
  file = {/Users/niels/Zotero/storage/8W6ZJ75Y/Tännsjö - 2008 - Understanding ethics an introduction to moral the.pdf},
  isbn = {978-0-7486-3690-7 978-0-7486-3689-1},
  language = {en},
  note = {OCLC: 254748111}
}

@article{thomson2001,
  title = {Ethics in Computer Software Design and Development},
  author = {Thomson, Alan J and Schmoldt, Daniel L},
  year = {2001},
  month = feb,
  volume = {30},
  pages = {85--102},
  issn = {01681699},
  doi = {10.1016/S0168-1699(00)00158-7},
  abstract = {Over the past 20 years, computer software has become integral and commonplace for operational and management tasks throughout agricultural and natural resource disciplines. During this software infusion, however, little thought has been afforded human impacts, both good and bad. This paper examines current ethical issues of software system design and development in relation to privacy, accuracy, property, accessibility, and effects on quality of life. These issues are explored in the context of simulation models, databases, geographic information systems and artificial intelligence programs, especially expert systems. New approaches to system development place a much higher emphasis on the effects of system deployment within a complex human environment. Software design decisions often depend on more than one ethical issue, possibly conflicting, where the appropriate ethical choice is not always clear cut. Professional codes of ethics do little to change peoples' behavior; rather, incentives for using an ethical approach to software development may lie in significantly increased likelihood of system success. Crown copyright \textcopyright{} 2001 Published by Elsevier Science B.V. All rights reserved.},
  file = {/Users/niels/Zotero/storage/ERVKGWRT/Thomson and Schmoldt - 2001 - Ethics in computer software design and development.pdf},
  journal = {Computers and Electronics in Agriculture},
  language = {en},
  number = {1-3}
}

@article{thomson2001a,
  title = {Ethics in Computer Software Design and Development},
  author = {Thomson, Alan J and Schmoldt, Daniel L},
  year = {2001},
  month = feb,
  volume = {30},
  pages = {85--102},
  issn = {01681699},
  doi = {10.1016/S0168-1699(00)00158-7},
  abstract = {Over the past 20 years, computer software has become integral and commonplace for operational and management tasks throughout agricultural and natural resource disciplines. During this software infusion, however, little thought has been afforded human impacts, both good and bad. This paper examines current ethical issues of software system design and development in relation to privacy, accuracy, property, accessibility, and effects on quality of life. These issues are explored in the context of simulation models, databases, geographic information systems and artificial intelligence programs, especially expert systems. New approaches to system development place a much higher emphasis on the effects of system deployment within a complex human environment. Software design decisions often depend on more than one ethical issue, possibly conflicting, where the appropriate ethical choice is not always clear cut. Professional codes of ethics do little to change peoples' behavior; rather, incentives for using an ethical approach to software development may lie in significantly increased likelihood of system success. Crown copyright \textcopyright{} 2001 Published by Elsevier Science B.V. All rights reserved.},
  file = {/Users/niels/Zotero/storage/CGFNMJ49/Thomson and Schmoldt - 2001 - Ethics in computer software design and development.pdf},
  journal = {Computers and Electronics in Agriculture},
  language = {en},
  number = {1-3}
}

@techreport{tsinghuauniversity,
  title = {China {{AI Development Report}} 2018},
  editor = {Tsinghua University},
  institution = {{China Institute for Science and Technology Policy at Tsinghua University}},
  file = {/Users/niels/Zotero/storage/8VNBS6SN/China_AI_development_report_2018.pdf}
}

@article{turchin,
  title = {Military {{AI}} as a {{Convergent Goal}} of {{Self}}-{{Improving AI}}},
  author = {Turchin, Alexey and Denkenberger, David},
  pages = {31},
  file = {/Users/niels/Zotero/storage/TDSBJPS6/Turchin and Denkenberger - Military AI as a Convergent Goal of Self-Improving.pdf},
  language = {en}
}

@article{vanwesel2017,
  title = {Challenges in the {{Verification}} of {{Reinforcement Learning Algorithms}}},
  author = {{van Wesel}, Perry},
  year = {2017},
  pages = {30},
  file = {/Users/niels/Zotero/storage/4SG3Z66V/van Wesel - 2017 - Challenges in the Veriﬁcation of Reinforcement Lea.pdf},
  language = {en}
}

@misc{vereintenationen1948,
  title = {{Resolution der Generalversammlung}},
  author = {Vereinte Nationen},
  year = {1948},
  month = dec,
  file = {/Users/niels/Zotero/storage/76UTWPFV/aemr.pdf},
  language = {de}
}

@article{walterbusch2012,
  title = {{Vertrauen im Cloud Computing}},
  author = {Walterbusch, Marc and Teuteberg, Frank},
  year = {2012},
  month = dec,
  volume = {49},
  pages = {50--59},
  issn = {1436-3011, 2198-2775},
  doi = {10.1007/BF03340757},
  file = {/Users/niels/Zotero/storage/PRRA9655/Walterbusch und Teuteberg - 2012 - Vertrauen im Cloud Computing.pdf},
  journal = {HMD Praxis der Wirtschaftsinformatik},
  language = {de},
  number = {6}
}

@book{werkner2017,
  title = {{Handbuch Friedensethik}},
  editor = {Werkner, Ines-Jacqueline and Ebeling, Klaus},
  year = {2017},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-14686-3},
  file = {/Users/niels/Zotero/storage/WSUYWKEK/Werkner und Ebeling - 2017 - Handbuch Friedensethik.pdf},
  isbn = {978-3-658-14685-6 978-3-658-14686-3},
  language = {de}
}

@book{wittpahl2019,
  title = {{K{\"u}nstliche Intelligenz: Technologie, Anwendung, Gesellschaft.}},
  shorttitle = {{K{\"u}nstliche Intelligenz}},
  author = {Wittpahl, Volker and {Springer-Verlag GmbH}},
  year = {2019},
  file = {/Users/niels/Zotero/storage/Y4XV7PLE/Wittpahl and Springer-Verlag GmbH - 2019 - Künstliche Intelligenz Technologie, Anwendung, Ge.pdf},
  isbn = {978-3-662-58041-7},
  language = {de},
  note = {OCLC: 1079417702}
}

@article{woodworth,
  title = {Preference {{Learning}} in {{Assistive Robotics}}: {{Observational Repeated Inverse Reinforcement Learning}}},
  author = {Woodworth, Bryce and Ferrari, Francesco and Zosa, Teofilo E and Riek, Laurel D},
  pages = {19},
  abstract = {As robots become more affordable and more common in everyday life, there will be an ever-increasing demand for adaptive behavior that is personalized to the individual needs of users. To accomplish this, robots will need to learn about their users' unique preferences through interaction. Current preference learning techniques lack the ability to infer long-term, task-independent preferences in realistic, interactive, incomplete-information settings. To address this gap, we introduce a novel preference-inference formulation, inspired by assistive robotics applications, in which a robot must infer these kinds of preferences based only on observing the user's behavior in various tasks. We then propose a candidate inference algorithm based on maximum-margin methods, and evaluate its performance in the context of robot-assisted prehabilitation. We find that the algorithm learns to predict aspects of the user's behavior as it is given more data, and that it shows strong convergence properties after a small number of iterations.},
  file = {/Users/niels/Zotero/storage/LIQLG968/Woodworth et al. - Preference Learning in Assistive Robotics Observa.pdf},
  journal = {Preference Learning},
  language = {en}
}

@article{xiao2018,
  title = {{{UAV Relay}} in {{VANETs Against Smart Jamming With Reinforcement Learning}}},
  author = {Xiao, Liang and Lu, Xiaozhen and Xu, Dongjin and Tang, Yuliang and Wang, Lei and Zhuang, Weihua},
  year = {2018},
  month = may,
  volume = {67},
  pages = {4087--4097},
  issn = {0018-9545, 1939-9359},
  doi = {10.1109/TVT.2018.2789466},
  abstract = {Frequency hopping-based antijamming techniques are not always applicable in vehicular ad hoc networks (VANETs) due to the high mobility of onboard units (OBUs) and the largescale network topology. In this paper, we use unmanned aerial vehicles (UAVs) to relay the message of an OBU and improve the communication performance of VANETs against smart jammers that observe the ongoing OBU and UAV communication status and even induce the UAV to use a specific relay strategy and then attack it accordingly. More specifically, the UAV relays the OBU message to another roadside unit (RSU) with a better radio transmission condition if the serving RSU is heavily jammed or interfered. The interactions between a UAV and a smart jammer are formulated as an antijamming UAV relay game, in which the UAV decides whether or not to relay the OBU message to another RSU, and the jammer observes the UAV and the VANET strategy and chooses the jamming power accordingly. The Nash equilibria of the UAV relay game are derived to reveal how the optimal UAV relay strategy depends on the transmit cost and the UAV channel model. A hotbooting policy hill climbing-based UAV relay strategy is proposed to help the VANET resist jamming in the dynamic game without being aware of the VANET model and the jamming model. Simulation results show that the proposed relay strategy can efficiently reduce the bit error rate of the OBU message and thus increase the utility of the VANET compared with a Q-learning-based scheme.},
  file = {/Users/niels/Zotero/storage/CQPYXFFR/Xiao et al. - 2018 - UAV Relay in VANETs Against Smart Jamming With Rei.pdf},
  journal = {IEEE Transactions on Vehicular Technology},
  language = {en},
  number = {5}
}

@inproceedings{yates2013,
  title = {Understanding the {{Impact}} of {{Policy}}, {{Regulation}} and {{Governance}} on {{Mobile Broadband Diffusion}}},
  booktitle = {2013 46th {{Hawaii International Conference}} on {{System Sciences}}},
  author = {Yates, David J. and Gulati, Girish J. Jeff and Weiss, Joseph W.},
  year = {2013},
  month = jan,
  pages = {2852--2861},
  publisher = {{IEEE}},
  address = {{Wailea, HI, USA}},
  doi = {10.1109/HICSS.2013.583},
  abstract = {Biases in AI and machine learning algorithms are presented and analyzed through two issues management frameworks with the aim of showing how ethical problems and dilemmas can evolve. While ``the singularity'' concept in AI is presently more predictive than actual, both benefits and damage that can result by failure to consider biases in the design and development of AI. Inclusivity and stakeholder awareness regarding potential ethical risks and issues need to be identified during the design of AI algorithms to ensure that the most vulnerable in societies are protected from harm.},
  file = {/Users/niels/Zotero/storage/638DE9A6/Yates et al. - 2013 - Understanding the Impact of Policy, Regulation and.pdf},
  isbn = {978-1-4673-5933-7 978-0-7695-4892-0},
  language = {en}
}

@article{you2019,
  title = {Advanced Planning for Autonomous Vehicles Using Reinforcement Learning and Deep Inverse Reinforcement Learning},
  author = {You, Changxi and Lu, Jianbo and Filev, Dimitar and Tsiotras, Panagiotis},
  year = {2019},
  month = apr,
  volume = {114},
  pages = {1--18},
  issn = {09218890},
  doi = {10.1016/j.robot.2019.01.003},
  abstract = {Autonomous vehicles promise to improve traffic safety while, at the same time, increase fuel efficiency and reduce congestion. They represent the main trend in future intelligent transportation systems. This paper concentrates on the planning problem of autonomous vehicles in traffic. We model the interaction between the autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an expert driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. The desired, expert-like driving behavior of the autonomous vehicle is obtained as follows: First, we design the reward function of the corresponding MDP and determine the optimal driving strategy for the autonomous vehicle using reinforcement learning techniques. Second, we collect a number of demonstrations from an expert driver and learn the optimal driving strategy based on data using inverse reinforcement learning. The unknown reward function of the expert driver is approximated using a deep neural-network (DNN). We clarify and validate the application of the maximum entropy principle (MEP) to learn the DNN reward function, and provide the necessary derivations for using the maximum entropy principle to learn a parameterized feature (reward) function. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle using both the reinforcement learning and inverse reinforcement learning techniques.},
  file = {/Users/niels/Zotero/storage/YR423UK2/You et al. - 2019 - Advanced planning for autonomous vehicles using re.pdf},
  journal = {Robotics and Autonomous Systems},
  language = {en}
}

@article{yu,
  title = {Deep {{Reinforcement Learning}} for {{Simulated Autonomous Vehicle Control}}},
  author = {Yu, April and {Palefsky-Smith}, Raphael and Bedi, Rishi},
  pages = {7},
  abstract = {We investigate the use of Deep Q-Learning to control a simulated car via reinforcement learning. We start by implementing the approach of [5] ourselves, and then experimenting with various possible alterations to improve performance on our selected task. In particular, we experiment with various reward functions to induce specific driving behavior, double Q-learning, gradient update rules, and other hyperparameters.},
  file = {/Users/niels/Zotero/storage/NIT8KU9A/Yu et al. - Deep Reinforcement Learning for Simulated Autonomo.pdf},
  language = {en}
}

@book{yusufi2014,
  title = {{Ethik im Weltkontext: Geschichten - Erscheinungsformen - neuere Konzepte}},
  shorttitle = {{Ethik im Weltkontext}},
  editor = {Y{\=u}suf{\=i}, {\d H}am{\=i}d Ri{\textsubumlaut z}{\=a} and Seubert, Harald},
  year = {2014},
  publisher = {{Springer VS}},
  address = {{Wiesbaden}},
  file = {/Users/niels/Zotero/storage/Z8GMUAZT/Yūsufī und Seubert - 2014 - Ethik im Weltkontext Geschichten - Erscheinungsfo.pdf},
  isbn = {978-3-658-04896-9},
  keywords = {Ethics,Religion and ethics,Religious ethics},
  language = {de},
  lccn = {BJ1114 .E768 2014}
}

@inbook{zerres2019,
  title = {{Schuldrecht \textendash{} Allgemeiner Teil}},
  booktitle = {{B{\"u}rgerliches Recht}},
  author = {Zerres, Thomas},
  year = {2019},
  pages = {111--213},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-58460-6_3},
  collaborator = {Zerres, Thomas},
  file = {/Users/niels/Zotero/storage/T4PRLL28/Zerres - 2019 - Schuldrecht – Allgemeiner Teil.pdf},
  isbn = {978-3-662-58459-0 978-3-662-58460-6},
  language = {de}
}

@article{zhang2015,
  title = {Geometric {{Reinforcement Learning}} for {{Path Planning}} of {{UAVs}}},
  author = {Zhang, Baochang and Mao, Zhili and Liu, Wanquan and Liu, Jianzhuang},
  year = {2015},
  month = feb,
  volume = {77},
  pages = {391--409},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-013-9901-z},
  file = {/Users/niels/Zotero/storage/LSVRLU4Q/Zhang et al. - 2015 - Geometric Reinforcement Learning for Path Planning.pdf},
  journal = {Journal of Intelligent \& Robotic Systems},
  language = {en},
  number = {2}
}

@misc{zotero-343,
  title = {Schwerpunkte\_{{KI}}-{{Strategien}}\_2019.Pdf},
  file = {/Users/niels/Zotero/storage/DGW5B5YW/Schwerpunkte_KI-Strategien_2019.pdf}
}

@misc{zotero-432,
  title = {{{ISO}} - {{ISO}}/{{IEC CD}} 22989 - {{Artificial}} Intelligence \textemdash{} {{Concepts}} and Terminology},
  howpublished = {https://www.iso.org/standard/74296.html}
}

@misc{zotero-447,
  title = {Statistic\_id428692\_size-of-the-Global-Autonomous-Car-Market-2018-2030.Pdf},
  file = {/Users/niels/Zotero/storage/8CE5JTA2/statistic_id428692_size-of-the-global-autonomous-car-market-2018-2030.pdf}
}


