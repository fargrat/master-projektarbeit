\section{Definition ethischer Werte}\label{sec:def_ethischer_werte}
Die Wahl ethischer Grundwerte für die Entwicklung und Anwendung von Reinforcement-Learning-Anwendungen wird im Folgenden auf Grundlage ethischer Prinzipien, sowie bestehender Standards und Normen begründet.
Ziel ist eine Konsolidierung bestehender Ansätze für die in \autoref{sec:massnahmen} vorgestellte Konzeptionierung eines praktischen Maßnahmekataloges.
Die Werte sollen dabei klar verständlich und möglichst allgemeingültig sein.
Überkulturelle Grundlage ist im Allgemeinen die Achtung der Menschenrechte \cite{vereintenationen1948}.
Oberstes Ziel sollte die Zusicherung und Wahrung eben dieser sein.
Zusätzlich macht es Sinn, ethische Werte zuzusischern, um neben der Unversehrtheit von Wohl und Würde des Menschen, auch Vertrauen in die Technologie aufzubauen.
Die Wahl der Werte wird in den einzelnen Unterkapiteln näher erläutert, basiert jedoch stark auf den in \autoref{chap:standards} vorgestellten Normen und Standards der einzelnen Regionen.
Um nicht von einzelnen Verfahren innerhalb der Technologie und Domäne abhängig zu sein, sind die Werte möglichst allgemein gehalten.
Bei der Definition auftretende Fragen sollen im Rahmen dieser Arbeit nicht unbedingt behandelt werden, sondern dienen viel mehr zu Veranschaulichung der Relevanz und der Problematik dieser Werte.
\subsection{Nachvollziehbarkeit und Erklärbarkeit}\label{sub:nachvollziehbarkeit}
Die ethische Entscheidungsfindung besteht laut \cite[S. 4]{tannsjo2018} aus zwei Komponenten.
Zum einen aus der Anwendung gelernter moralischer Prinzipien und zum anderen aus der Hinzunahme möglichst aller relevanter Informationen.
Dieser Prozess der ethischen Entscheidungsfindung ist insbesondere bei der Bewertung eines Agenten relevant.
Ein Agent soll gemäß unserer menschlichen Werte handeln und muss deshalb auch gemäß dieser bewertet werden.
Nachvollziehbarkeit bezieht sich dabei zum einen auf die Kenntnis über vergangene und möglicherweise die Absicht der Ausführung zukünftiger Aktionen und zum anderen auf die Fähigkeiten und Limitierungen des Systems, sowie auf den Entstehungsprozess.
Insbesondere für Entscheidungsprozesse von domänenfremden Verantwortungsträgern müssen Güte und Entstehungsprozess nachvollziehbar sein, dadurch dass Weiterentwicklung und Optimierung darauf basieren.
Erklärbarkeit fordert zusätzlich, dass Entscheidungen nicht nur nachvollzogen, sondern begründet werden können.
Die Werte Nachvollziehbarkeit und Erklärbarkeit bieten die Grundlage für die Zusicherung der folgenden zwei Werte.
So kann das in \autoref{sub:vertrauen} geforderte Vertrauen nur dann aufgebaut werden, wenn durch das System ein Verständnis seitens der verantwortlichen Personen, insbesondere des Entwicklers über das Handeln entsteht und Personen im Anwendungskontext auf Grund der Erklärbarkeit ihr Handeln ggfs. anpassen und das des Agenten einschätzen können.
Die Frage der in \autoref{sub:verantwortung} beschriebenen Verantwortung basiert zudem auf der Grundlage, dass alle verantwortwortlichen Personen das Verhalten des Agenten kennen, nachvollziehen können und der Prozess der Entscheidungsfindung mit den persönlichen Werten übereinstimmt.

\subsection{Vertrauen durch Kalkulierbarkeit und Zuverlässigkeit}\label{sub:vertrauen}
Vertrauen ist insbesondere bei der Adaption ein wichtiger Treiber und damit Grundlage des Fortschrittes der hier betrachteten Technologien.
Vertrauen entsteht dann, wenn alle beteiligten Parteien eine gemeinsame \enquote{[...] Basis geteilter Normen, Werte und positiver Zukunftserwartungen [...]}\cite[S. 61]{gilberta} besitzen.
Es ermöglicht Kalkulierbarkeit unabhängig von rechtlichen Verpflichtungen \cite[S. 61 ff.]{gilberta}.
Vertrauen kann in diesem Anwendungsbereich in der Regel nicht in einzelne Personen aufgebaut werden, da bei komplexen Softwaresystemen oft große Teams zum Einsatz kommen und das Produkt somit Ergebnis der Zusammenarbeit vieler ist.
Deshalb muss das Unternehmen als ganzes ein Systemvertrauen aufbauen, indem Anwender auf die Prinzipien des Unternehmens vertrauen können.
Zuverlässigkeit in Form von Korrektheit und Robustheit in unbekannten Situationen können helfen Vertrauen langfristig aufzubauen \cite[S. 13]{avizienis2004}.
Die Anforderungen des Systems, sowie das tatsächliche Handeln müssen zum Aufbau von Vertrauen mit den Erwartungen übereinstimmen und können nur erfüllt werden, wenn das System kalkulierbar handelt.

\subsection{Verantwortung und Schuld}\label{sub:verantwortung}
Sobald Agenten Einfluss auf Menschen nehmen können, muss geklärt werden, wer Verantwortung für die Folgen des Handelns eines Agenten und damit die Schuld im Fehlerfall trägt \cite[S. 88]{heckman1998}.
Verantwortung ist dabei die notwendige Bedingungen, um tatsächlich Schuld zuzuweisen \cite[S. 75]{duttge2009}.
Die Verantwortung wird zum einen von der Umwelt gefordert \cite[S. 1]{hoffrage2019} und zum anderen von Innen durch das persönliche Gewissen der Verantwortlichen erbracht.
Durch korrekte Dokumentation kann für einzelne Teile eines Systems zwar die Frage der verantwortlichen Person ermittelt werden, gerade bei selbstlernenden Systemen sind die verursachenden Verhaltensweisen jedoch nicht explizit implementiert, sondern entstehen möglicherweise als unerwünschte Randeffekte.
Deshalb ist die Frage der Verantwortung und Schuld insbesondere bei selbstlernenden Systemen nicht trivial und sollte im Laufe des Produktlebenszyklus betrachtet werden.
Eine Bestrafung im Sinne der Schuld trägt bei Maschinen keine Wirkung.
Die Frage der Schuld betriftt also die Verantwortlichen.
Laut \cite[S. 76]{duttge2009} \enquote{[...] kann man niemanden für die Verletzung einer Norm zur Rechenschaft ziehen, die er gar nicht einhalten konnte [...]}.
Im Bezug zu selbstlernenden und insbesondere autonomen Systemen stellt sich also die Frage, ob ein Verantwortlicher die Verletzung der Norm beeinflussen kann.
So ist beispielsweise ein Kriegseinsatz nur dann rechtmäßig, wenn die Verantwortung für mögliche Tode eindeutig geklärt ist.
Zu klären bleibt dann, inwiefern ein Einsatz gerechtfertigt ist, wenn z.B. ein autonomes Waffensystem zwar weniger Fehler macht als Menschen, die Schuldfrage jedoch nicht eindeutig geklärt werden kann.
